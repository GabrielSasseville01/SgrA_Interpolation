{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on Random Walk\n",
    "\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint_trainer.py --niters 300 --batch-size 260 --dataset randomwalk --norm --shuffle --sample-tp 0.1 --mse-weight 1.0 --imab-dim 64 --cab-dim 256 --decoder-dim 128 --nlayers 1 --sample-type random --num-ref-points 128\n",
      "(50000, 200, 9) (25000, 200, 9) (25000, 200, 9)\n",
      "/home/gsasseville/.local/share/virtualenvs/Tripletformer-nUF7tw2u/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "No checkpoint file found. Training from Epoch 1\n",
      "Experiment ID 3456490\n"
     ]
    }
   ],
   "source": [
    "!python checkpoint_trainer.py --niters 300 --batch-size 260 --dataset randomwalk --norm --shuffle --sample-tp 0.1 --mse-weight 1.0 --imab-dim 64 --cab-dim 256 --decoder-dim 128 --nlayers 1 --sample-type random --num-ref-points 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_prediction.py --batch-size 60 --dataset randomwalk --norm --shuffle --sample-tp 0.1 --mse-weight 1.0 --imab-dim 64 --cab-dim 256 --decoder-dim 128 --nlayers 1 --sample-type random --num-ref-points 128 --experiment-id 9821876\n",
      "(50000, 200, 9) (25000, 200, 9) (25000, 200, 9)\n",
      "test_prediction.py:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  chp = torch.load(f'./saved_models/{args.dataset}_{args.experiment_id}.h5')\n",
      "Number of points to predict:  tensor(332, device='cuda:0')\n",
      "Length of means 332\n",
      "Length of channel indices 332\n",
      "Interpolation of Channel:  0\n",
      "Amount of points to predict:  85 for channel:  0\n",
      "Length of means:  85\n",
      "Most frequent value: -8.330575\n",
      "Count: 1\n",
      "Mean of the means:  0.9360391\n",
      "Time indices of artefacts:  [198]\n",
      "Length of this array of indices:  1\n",
      "Interpolation of Channel:  1\n",
      "Amount of points to predict:  87 for channel:  1\n",
      "Length of means:  87\n",
      "Most frequent value: -4.7673926\n",
      "Count: 1\n",
      "Mean of the means:  0.03860485\n",
      "Time indices of artefacts:  [84]\n",
      "Length of this array of indices:  1\n",
      "Interpolation of Channel:  2\n",
      "Amount of points to predict:  85 for channel:  2\n",
      "Length of means:  85\n",
      "Most frequent value: -2.6776052\n",
      "Count: 1\n",
      "Mean of the means:  3.3821912\n",
      "Time indices of artefacts:  [51]\n",
      "Length of this array of indices:  1\n",
      "Interpolation of Channel:  3\n",
      "Amount of points to predict:  75 for channel:  3\n",
      "Length of means:  75\n",
      "Most frequent value: -23.42911\n",
      "Count: 1\n",
      "Mean of the means:  -11.238913\n",
      "Time indices of artefacts:  [198]\n",
      "Length of this array of indices:  1\n",
      "Total pred points is: 332\n"
     ]
    }
   ],
   "source": [
    "!python test_prediction.py --batch-size 260 --dataset randomwalk --norm --shuffle --sample-tp 0.1 --mse-weight 1.0 --imab-dim 64 --cab-dim 256 --decoder-dim 128 --nlayers 1 --sample-type random --num-ref-points 128 --experiment-id 9821876"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on Full Data\n",
    "\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python checkpoint_trainer.py --niters 2000 --batch-size 20 --dataset sgra --norm --shuffle --sample-tp 0.1 --mse-weight 1.0 --imab-dim 64 --cab-dim 256 --decoder-dim 128 --nlayers 1 --sample-type random --num-ref-points 128 --experiment-id 982081"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_prediction.py --niters 1 --batch-size 20 --dataset sgrA --norm --shuffle --sample-tp 0.1 --mse-weight 1.0 --imab-dim 64 --cab-dim 256 --decoder-dim 128 --nlayers 1 --sample-type random --num-ref-points 128 --experiment-id 982081\n",
      "(6300, 960, 9) (1260, 960, 9) (840, 960, 9)\n",
      "test_prediction.py:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  chp = torch.load(f'./saved_models/{args.dataset}_{args.experiment_id}.h5')\n",
      "Number of points to predict:  tensor(1967, device='cuda:0')\n",
      "Length of means 1967\n",
      "Length of channel indices 1967\n",
      "Interpolation of Channel:  0\n",
      "Amount of points to predict:  771 for channel:  0\n",
      "Length of means:  771\n",
      "Most frequent value: -0.077152\n",
      "Count: 156\n",
      "Mean of the means:  -0.15843123\n",
      "Time indices of artefacts:  [  0  25  27  28  44  46  52  53  57  64  65  71  73  74  76  79  88  90\n",
      " 152 163 165 175 179 182 188 189 191 208 210 212 230 249 250 254 266 273\n",
      " 276 277 303 305 311 328 337 341 343 358 365 390 392 419 434 435 437 441\n",
      " 447 460 472 477 486 504 505 506 513 525 535 536 550 567 568 574 578 581\n",
      " 582 583 587 592 598 604 608 610 617 619 630 640 654 655 666 668 671 673\n",
      " 674 682 687 693 694 695 696 700 701 708 709 722 730 735 737 741 742 743\n",
      " 747 750 751 754 757 759 760 763 764 766 770 780 786 787 791 794 802 809\n",
      " 816 821 822 824 825 837 841 842 848 853 854 856 858 871 878 879 885 897\n",
      " 906 911 916 919 922 928 933 942 946 951 952 953]\n",
      "Length of this array of indices:  156\n",
      "Interpolation of Channel:  1\n",
      "Amount of points to predict:  192 for channel:  1\n",
      "Length of means:  192\n",
      "Most frequent value: -0.19876918\n",
      "Count: 53\n",
      "Mean of the means:  -0.038088035\n",
      "Time indices of artefacts:  [ 27  28  44  57  63  71  88 105 115 152 165 171 183 188 201 241 250 255\n",
      " 266 277 296 358 365 390 429 430 477 504 550 598 604 630 640 666 674 695\n",
      " 701 730 735 737 742 743 750 751 766 791 802 821 841 853 854 875 922]\n",
      "Length of this array of indices:  53\n",
      "Interpolation of Channel:  2\n",
      "Amount of points to predict:  812 for channel:  2\n",
      "Length of means:  812\n",
      "Most frequent value: -0.38460296\n",
      "Count: 180\n",
      "Mean of the means:  -0.39924195\n",
      "Time indices of artefacts:  [  0  25  27  28  44  46  52  53  57  64  65  71  73  74  76  79  88  90\n",
      "  92 147 148 165 171 172 182 188 189 191 198 201 208 210 212 215 236 238\n",
      " 241 249 250 254 273 276 277 291 296 298 303 305 307 310 311 314 322 325\n",
      " 328 332 341 343 365 367 379 390 392 404 419 434 435 437 441 444 447 460\n",
      " 462 472 496 505 506 513 514 535 536 541 550 567 568 578 581 582 583 587\n",
      " 592 598 604 608 610 617 619 630 640 654 655 666 668 671 673 674 682 687\n",
      " 693 694 695 696 700 701 708 709 722 730 735 737 741 742 743 747 750 751\n",
      " 754 757 759 760 763 764 766 770 777 778 780 786 787 791 794 802 806 807\n",
      " 808 809 816 821 822 824 825 827 828 829 837 841 842 848 853 854 856 858\n",
      " 871 878 879 885 897 906 910 911 916 919 922 928 933 942 946 951 952 953]\n",
      "Length of this array of indices:  180\n",
      "Interpolation of Channel:  3\n",
      "Amount of points to predict:  192 for channel:  3\n",
      "Length of means:  192\n",
      "Most frequent value: -0.20903042\n",
      "Count: 192\n",
      "/home/gsasseville/.local/share/virtualenvs/Tripletformer-nUF7tw2u/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/gsasseville/.local/share/virtualenvs/Tripletformer-nUF7tw2u/lib/python3.8/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Mean of the means:  nan\n",
      "Time indices of artefacts:  [ 25  27  28  44  46  52  53  57  64  65  71  73  74  76  79  88  90  92\n",
      " 105 115 129 147 148 152 163 165 171 172 175 182 188 189 191 198 201 208\n",
      " 210 212 215 230 236 238 241 242 249 250 254 255 258 266 273 276 277 284\n",
      " 291 296 298 303 305 310 311 314 322 325 328 332 335 337 341 343 358 365\n",
      " 367 379 388 390 392 404 419 434 435 437 441 444 447 460 462 472 477 486\n",
      " 496 499 504 505 506 511 513 514 523 525 535 536 541 550 567 568 578 581\n",
      " 582 583 587 592 598 604 608 610 617 619 630 640 654 655 666 668 671 673\n",
      " 674 682 687 693 694 695 696 700 701 708 709 722 730 735 737 741 742 743\n",
      " 747 750 751 754 757 759 760 763 764 766 770 780 786 787 791 794 802 809\n",
      " 816 821 822 824 825 837 841 842 848 853 854 856 858 871 878 879 885 897\n",
      " 906 911 916 919 922 928 933 942 946 951 952 953]\n",
      "Length of this array of indices:  192\n",
      "Total pred points is: 1967\n"
     ]
    }
   ],
   "source": [
    "!python test_prediction.py --niters 1 --batch-size 20 --dataset sgrA --norm --shuffle --sample-tp 0.1 --mse-weight 1.0 --imab-dim 64 --cab-dim 256 --decoder-dim 128 --nlayers 1 --sample-type random --num-ref-points 128 --experiment-id 982081"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artefacts Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing elements in array 1: [  0  39 167 610 642 889]\n",
      "All elements of array 2 are found in the main array.\n",
      "Missing elements in array 3: [  0  34 426 950]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Main array of integers\n",
    "main_array = np.array([\n",
    "    1, 3, 5, 17, 23, 25, 26, 28, 30, 37, 43, 47, 48, 67, 69, 75, 82, 83,\n",
    "    92, 94, 98, 100, 101, 113, 119, 121, 123, 124, 127, 131, 134, 136,\n",
    "    142, 150, 154, 156, 161, 162, 165, 177, 180, 186, 197, 199, 206, 209,\n",
    "    212, 215, 216, 217, 221, 222, 226, 231, 233, 261, 287, 293, 297, 302,\n",
    "    303, 309, 311, 318, 319, 324, 326, 327, 332, 344, 347, 357, 378, 381,\n",
    "    382, 383, 385, 387, 388, 390, 391, 393, 398, 400, 407, 416, 422, 429,\n",
    "    431, 437, 441, 445, 455, 466, 470, 472, 488, 494, 500, 503, 513, 515,\n",
    "    520, 524, 525, 527, 531, 533, 534, 537, 548, 553, 567, 574, 576, 577,\n",
    "    580, 582, 586, 588, 598, 602, 603, 604, 616, 624, 626, 627, 628, 633,\n",
    "    643, 651, 657, 666, 680, 711, 712, 713, 721, 727, 728, 729, 730, 731,\n",
    "    733, 735, 738, 747, 757, 758, 767, 776, 777, 778, 782, 784, 789, 803,\n",
    "    806, 807, 809, 810, 812, 817, 829, 833, 835, 838, 839, 859, 861, 868,\n",
    "    870, 872, 883, 884, 885, 887, 892, 895, 900, 901, 903, 904, 907, 908,\n",
    "    910, 911, 925, 937, 940, 951\n",
    "])\n",
    "\n",
    "# Arrays to check\n",
    "arrays_to_check = [\n",
    "    np.array([\n",
    "        0, 1, 3, 5, 17, 23, 25, 26, 28, 30, 37, 39, 43, 47, 48, 67, 69, 75,\n",
    "        82, 83, 92, 94, 98, 100, 101, 113, 119, 121, 123, 124, 127, 131, 134,\n",
    "        136, 142, 150, 154, 156, 161, 162, 165, 167, 177, 180, 186, 197, 199,\n",
    "        206, 209, 212, 215, 216, 217, 221, 222, 226, 231, 233, 261, 287, 293,\n",
    "        297, 302, 303, 309, 311, 318, 319, 324, 326, 327, 347, 357, 378, 387,\n",
    "        388, 390, 391, 400, 416, 422, 429, 431, 437, 441, 445, 455, 466, 470,\n",
    "        494, 503, 515, 524, 525, 527, 531, 533, 534, 537, 548, 553, 567, 574,\n",
    "        586, 588, 598, 603, 610, 627, 633, 642, 643, 651, 657, 666, 680, 711,\n",
    "        712, 713, 727, 735, 738, 747, 757, 758, 767, 776, 777, 778, 782, 784,\n",
    "        789, 803, 806, 807, 809, 810, 812, 817, 829, 833, 835, 838, 839, 859,\n",
    "        861, 868, 870, 872, 883, 884, 885, 887, 889, 892, 895, 900, 901, 903,\n",
    "        904, 907, 908, 910, 911, 925, 937, 940, 951\n",
    "    ]),\n",
    "    np.array([\n",
    "        5, 25, 98, 119, 123, 127, 131, 206, 209, 261, 287, 318, 319, 327,\n",
    "        382, 400, 466, 470, 494, 525, 527, 537, 576, 577, 580, 586, 588,\n",
    "        604, 666, 757, 767, 784, 838, 859, 895, 937\n",
    "    ]),\n",
    "    np.array([\n",
    "        0, 1, 3, 5, 17, 23, 25, 26, 28, 30, 34, 37, 43, 47, 48, 67, 69, 75,\n",
    "        82, 83, 92, 94, 98, 100, 101, 113, 119, 121, 123, 124, 127, 131, 134,\n",
    "        136, 142, 150, 154, 156, 161, 162, 165, 177, 180, 186, 197, 199, 206,\n",
    "        209, 212, 215, 216, 217, 221, 222, 226, 231, 233, 261, 287, 293, 297,\n",
    "        302, 303, 309, 311, 318, 319, 324, 326, 327, 332, 344, 357, 381, 382,\n",
    "        383, 385, 387, 391, 393, 398, 407, 416, 426, 429, 431, 437, 441, 455,\n",
    "        466, 470, 472, 488, 494, 503, 513, 515, 520, 524, 525, 527, 534, 548,\n",
    "        580, 586, 588, 602, 603, 604, 616, 624, 628, 633, 657, 711, 712, 713,\n",
    "        721, 727, 728, 730, 731, 738, 757, 767, 776, 778, 782, 784, 789, 803,\n",
    "        806, 807, 809, 810, 812, 817, 829, 833, 835, 838, 839, 859, 861, 868,\n",
    "        870, 872, 883, 884, 885, 887, 892, 895, 900, 901, 903, 904, 907, 908,\n",
    "        910, 911, 925, 937, 940, 950, 951\n",
    "    ])\n",
    "]\n",
    "\n",
    "# Check each array and find missing elements\n",
    "for idx, array in enumerate(arrays_to_check):\n",
    "    missing_elements = array[~np.isin(array, main_array)]\n",
    "    if len(missing_elements) > 0:\n",
    "        print(f\"Missing elements in array {idx + 1}: {missing_elements}\")\n",
    "    else:\n",
    "        print(f\"All elements of array {idx + 1} are found in the main array.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on Full Data Without X-ray\n",
    "\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint_trainer.py --niters 200 --batch-size 50 --dataset noxray --norm --shuffle --sample-tp 0.1 --mse-weight 1.0 --imab-dim 64 --cab-dim 256 --decoder-dim 128 --nlayers 1 --sample-type random --num-ref-points 128 --experiment-id 4084951\n",
      "(2610, 960, 7) (1392, 960, 7) (348, 960, 7)\n",
      "/home/gsasseville/.local/share/virtualenvs/Tripletformer-nUF7tw2u/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "checkpoint_trainer.py:70: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "Resuming training from checkpoint at Epoch 18\n",
      "Experiment ID 4084951\n",
      "Logvar is negative\n",
      "Logvar is negative\n",
      "Logvar is negative\n",
      "Logvar is negative\n",
      "Logvar is negative\n",
      "Logvar is negative\n",
      "Logvar is negative\n",
      "Logvar is negative\n",
      "Logvar is negative\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"checkpoint_trainer.py\", line 96, in <module>\n",
      "    loss_info = net.compute_unsupervised_loss(\n",
      "  File \"/home/gsasseville/Files/UDEM/Maitrise/SgrA/SgrA_Interpolation/Tripletformer/tripletformer.py\", line 96, in compute_unsupervised_loss\n",
      "    C = torch.ones(mk.size(), dtype=torch.int64).cumsum(-1) - 1 \n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python checkpoint_trainer.py --niters 200 --batch-size 50 --dataset noxray --norm --shuffle --sample-tp 0.1 --mse-weight 1.0 --imab-dim 64 --cab-dim 256 --decoder-dim 128 --nlayers 1 --sample-type random --num-ref-points 128 --experiment-id 4084951"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_prediction.py --niters 1 --batch-size 60 --dataset noxray --norm --shuffle --sample-tp 0.1 --mse-weight 1.0 --imab-dim 64 --cab-dim 256 --decoder-dim 128 --nlayers 1 --sample-type random --num-ref-points 128 --experiment-id 4084951\n",
      "(2610, 960, 7) (1392, 960, 7) (348, 960, 7)\n",
      "test_prediction.py:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  chp = torch.load(f'./saved_models/{args.dataset}_{args.experiment_id}.h5')\n",
      "Number of points to predict:  tensor(1806, device='cuda:0')\n",
      "Length of means 1806\n",
      "Length of channel indices 1806\n",
      "Interpolation of Channel:  0\n",
      "Amount of points to predict:  802 for channel:  0\n",
      "Length of means:  802\n",
      "Most frequent value: -0.0058916584\n",
      "Count: 802\n",
      "/home/gsasseville/.local/share/virtualenvs/Tripletformer-nUF7tw2u/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/gsasseville/.local/share/virtualenvs/Tripletformer-nUF7tw2u/lib/python3.8/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Mean of the means:  nan\n",
      "Time indices of artefacts:  [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 144\n",
      " 145 146 147 148 149 150 151 152 157 158 159 160 161 162 163 164 165 166\n",
      " 171 173 174 175 176 177 178 182 184 185 186 187 188 189 190 191 192 193\n",
      " 197 198 199 200 201 202 203 206 209 210 213 214 215 216 217 218 219 220\n",
      " 222 227 228 229 230 231 232 233 234 235 243 244 246 254 255 260 261 262\n",
      " 263 264 265 266 267 268 271 277 278 279 283 287 288 289 290 291 292 293\n",
      " 294 295 296 300 303 304 311 314 315 316 317 318 319 320 322 323 326 327\n",
      " 328 329 330 331 332 336 337 339 340 341 342 343 344 345 353 354 355 358\n",
      " 360 361 362 363 365 366 368 369 372 375 376 377 385 386 387 388 389 390\n",
      " 397 398 399 400 403 404 405 406 407 408 409 410 411 412 413 416 417 418\n",
      " 419 420 421 422 423 424 425 426 427 438 439 440 441 442 443 444 446 447\n",
      " 448 452 457 458 459 460 461 462 463 464 474 475 476 477 478 479 485 487\n",
      " 488 489 490 491 492 493 494 496 497 498 499 501 505 508 512 513 514 515\n",
      " 516 517 518 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535\n",
      " 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553\n",
      " 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571\n",
      " 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589\n",
      " 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607\n",
      " 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625\n",
      " 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643\n",
      " 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661\n",
      " 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679\n",
      " 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697\n",
      " 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715\n",
      " 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733\n",
      " 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751\n",
      " 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769\n",
      " 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787\n",
      " 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805\n",
      " 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823\n",
      " 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841\n",
      " 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859\n",
      " 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877\n",
      " 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895\n",
      " 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913\n",
      " 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931\n",
      " 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949\n",
      " 950 951 952 953 954 955 956 957 958 959]\n",
      "Length of this array of indices:  802\n",
      "Interpolation of Channel:  1\n",
      "Amount of points to predict:  192 for channel:  1\n",
      "Length of means:  192\n",
      "Most frequent value: -0.008612432\n",
      "Count: 192\n",
      "Mean of the means:  nan\n",
      "Time indices of artefacts:  [  7   9  13  14  18  20  21  27  42  52  53  60  65  67  74  79  87  89\n",
      "  96 101 102 117 126 143 144 153 155 160 163 174 176 182 186 188 190 196\n",
      " 201 202 212 214 218 220 225 230 243 245 246 251 262 266 269 273 276 277\n",
      " 279 281 282 303 314 325 326 338 350 356 359 364 369 382 383 386 391 392\n",
      " 404 410 413 414 416 419 427 432 436 442 451 454 462 469 473 474 477 482\n",
      " 483 487 492 494 505 508 515 521 530 536 541 546 554 559 564 577 580 584\n",
      " 585 587 589 596 600 601 603 608 609 610 612 614 616 621 627 628 637 639\n",
      " 642 643 645 646 650 659 661 662 673 676 678 686 694 697 698 702 708 710\n",
      " 719 720 740 744 747 756 765 767 769 775 776 781 786 788 789 807 808 816\n",
      " 828 834 841 844 853 854 862 866 868 870 880 882 884 891 894 900 901 902\n",
      " 904 908 920 923 931 932 937 946 949 951 954 958]\n",
      "Length of this array of indices:  192\n",
      "Interpolation of Channel:  2\n",
      "Amount of points to predict:  812 for channel:  2\n",
      "Length of means:  812\n",
      "Most frequent value: 0.6894489\n",
      "Count: 381\n",
      "Mean of the means:  0.7075105\n",
      "Time indices of artefacts:  [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 140 141 142 143 144 152 153 154 155 156 157\n",
      " 164 165 166 167 168 174 176 177 178 179 180 188 189 190 191 192 193 194\n",
      " 200 201 202 203 204 207 209 210 211 212 213 214 215 216 217 218 219 220\n",
      " 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238\n",
      " 239 240 241 242 243 244 245 246 247 248 256 257 258 259 260 268 269 270\n",
      " 271 272 273 275 280 281 282 283 284 290 292 293 294 295 296 297 303 304\n",
      " 305 306 307 308 310 311 313 314 316 317 318 319 320 321 324 325 326 327\n",
      " 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345\n",
      " 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363\n",
      " 364 368 371 372 373 374 375 376 377 381 384 385 386 387 388 395 396 397\n",
      " 398 399 400 403 407 408 409 410 411 412 414 415 420 421 422 423 424 432\n",
      " 433 434 435 436 441 442 443 444 445 446 447 448 449 450 451 452 453 454\n",
      " 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472\n",
      " 473 474 475 476 477 478 479 480 481 486 487 488 489 490 491 492 500 501\n",
      " 502 503 504]\n",
      "Length of this array of indices:  381\n",
      "Total pred points is: 1806\n"
     ]
    }
   ],
   "source": [
    "!python test_prediction.py --niters 1 --batch-size 60 --dataset noxray --norm --shuffle --sample-tp 0.1 --mse-weight 1.0 --imab-dim 64 --cab-dim 256 --decoder-dim 128 --nlayers 1 --sample-type random --num-ref-points 128 --experiment-id 4084951"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on X-ray Only\n",
    "\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint_trainer.py --niters 200 --batch-size 240 --dataset xray --norm --shuffle --sample-tp 0.1 --mse-weight 1000 --imab-dim 64 --cab-dim 256 --decoder-dim 128 --nlayers 1 --sample-type random --num-ref-points 128\n",
      "(2400, 960, 3) (1280, 960, 3) (320, 960, 3)\n",
      "/home/gsasseville/.local/share/virtualenvs/Tripletformer-nUF7tw2u/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "No checkpoint file found. Training from Epoch 1\n",
      "Experiment ID 7935467\n",
      "\n",
      "Epoch 1 completed\n",
      "Training loss: 176914.9172\n",
      "NLL: 111.1389\n",
      "MSE: 176.8038\n",
      "Validation loss: 84.2589\n",
      "\n",
      "Epoch 2 completed\n",
      "Training loss: 156636.9047\n",
      "NLL: 68.5293\n",
      "MSE: 156.5684\n",
      "Validation loss: 46.5389\n",
      "\n",
      "Epoch 3 completed\n",
      "Training loss: 133961.2430\n",
      "NLL: 33.8842\n",
      "MSE: 133.9274\n",
      "Validation loss: 21.7642\n",
      "\n",
      "Epoch 4 completed\n",
      "Training loss: 125827.0273\n",
      "NLL: 20.5540\n",
      "MSE: 125.8065\n",
      "Validation loss: 19.8795\n",
      "\n",
      "Epoch 5 completed\n",
      "Training loss: 123267.4187\n",
      "NLL: 19.9405\n",
      "MSE: 123.2475\n",
      "Validation loss: 19.4853\n",
      "\n",
      "Epoch 6 completed\n",
      "Training loss: 122480.3430\n",
      "NLL: 18.3226\n",
      "MSE: 122.4620\n",
      "Validation loss: 15.9441\n",
      "\n",
      "Epoch 7 completed\n",
      "Training loss: 123229.6102\n",
      "NLL: 15.2265\n",
      "MSE: 123.2144\n",
      "Validation loss: 15.6783\n",
      "\n",
      "Epoch 8 completed\n",
      "Training loss: 122533.1258\n",
      "NLL: 15.8613\n",
      "MSE: 122.5173\n",
      "Validation loss: 15.6705\n",
      "\n",
      "Epoch 9 completed\n",
      "Training loss: 122268.8383\n",
      "NLL: 14.6918\n",
      "MSE: 122.2541\n",
      "Validation loss: 14.0659\n",
      "\n",
      "Epoch 10 completed\n",
      "Training loss: 120745.1609\n",
      "NLL: 14.6283\n",
      "MSE: 120.7305\n",
      "Validation loss: 13.4739\n",
      "\n",
      "Epoch 11 completed\n",
      "Training loss: 120416.8492\n",
      "NLL: 13.9289\n",
      "MSE: 120.4029\n",
      "Validation loss: 13.4628\n",
      "\n",
      "Epoch 12 completed\n",
      "Training loss: 120086.9719\n",
      "NLL: 13.7168\n",
      "MSE: 120.0733\n",
      "Validation loss: 14.1211\n",
      "\n",
      "Epoch 13 completed\n",
      "Training loss: 119641.8391\n",
      "NLL: 13.4627\n",
      "MSE: 119.6284\n",
      "Validation loss: 12.5859\n",
      "\n",
      "Epoch 14 completed\n",
      "Training loss: 119204.7664\n",
      "NLL: 12.9525\n",
      "MSE: 119.1918\n",
      "Validation loss: 11.8573\n",
      "\n",
      "Epoch 15 completed\n",
      "Training loss: 118787.3414\n",
      "NLL: 12.2339\n",
      "MSE: 118.7751\n",
      "Validation loss: 11.7707\n",
      "\n",
      "Epoch 16 completed\n",
      "Training loss: 118555.3508\n",
      "NLL: 12.0500\n",
      "MSE: 118.5433\n",
      "Validation loss: 11.4987\n",
      "\n",
      "Epoch 17 completed\n",
      "Training loss: 118559.0828\n",
      "NLL: 11.6689\n",
      "MSE: 118.5474\n",
      "Validation loss: 11.6464\n",
      "\n",
      "Epoch 18 completed\n",
      "Training loss: 118641.4008\n",
      "NLL: 11.4146\n",
      "MSE: 118.6300\n",
      "Validation loss: 11.1753\n",
      "\n",
      "Epoch 19 completed\n",
      "Training loss: 118397.1195\n",
      "NLL: 11.2462\n",
      "MSE: 118.3859\n",
      "Validation loss: 10.7510\n",
      "\n",
      "Epoch 20 completed\n",
      "Training loss: 118346.9039\n",
      "NLL: 10.8148\n",
      "MSE: 118.3361\n",
      "Validation loss: 10.4671\n",
      "\n",
      "Epoch 21 completed\n",
      "Training loss: 118408.7680\n",
      "NLL: 10.5815\n",
      "MSE: 118.3982\n",
      "Validation loss: 10.1275\n",
      "\n",
      "Epoch 22 completed\n",
      "Training loss: 118429.6352\n",
      "NLL: 10.1427\n",
      "MSE: 118.4195\n",
      "Validation loss: 9.8458\n",
      "\n",
      "Epoch 23 completed\n",
      "Training loss: 118329.3219\n",
      "NLL: 10.1888\n",
      "MSE: 118.3191\n",
      "Validation loss: 9.8863\n",
      "\n",
      "Epoch 24 completed\n",
      "Training loss: 118279.0305\n",
      "NLL: 10.0191\n",
      "MSE: 118.2690\n",
      "Validation loss: 9.6080\n",
      "\n",
      "Epoch 25 completed\n",
      "Training loss: 118307.1195\n",
      "NLL: 9.8101\n",
      "MSE: 118.2973\n",
      "Validation loss: 9.5456\n",
      "\n",
      "Epoch 26 completed\n",
      "Training loss: 118318.2234\n",
      "NLL: 9.8296\n",
      "MSE: 118.3084\n",
      "Validation loss: 9.2732\n",
      "\n",
      "Epoch 27 completed\n",
      "Training loss: 118354.9914\n",
      "NLL: 9.7497\n",
      "MSE: 118.3452\n",
      "Validation loss: 9.2743\n",
      "\n",
      "Epoch 28 completed\n",
      "Training loss: 118388.0078\n",
      "NLL: 9.5139\n",
      "MSE: 118.3785\n",
      "Validation loss: 9.4909\n",
      "\n",
      "Epoch 29 completed\n",
      "Training loss: 118270.8406\n",
      "NLL: 9.4794\n",
      "MSE: 118.2614\n",
      "Validation loss: 9.2345\n",
      "\n",
      "Epoch 30 completed\n",
      "Training loss: 118244.2023\n",
      "NLL: 9.3816\n",
      "MSE: 118.2348\n",
      "Validation loss: 9.2351\n",
      "\n",
      "Epoch 31 completed\n",
      "Training loss: 118239.7172\n",
      "NLL: 9.2364\n",
      "MSE: 118.2305\n",
      "Validation loss: 8.8988\n",
      "\n",
      "Epoch 32 completed\n",
      "Training loss: 118253.2539\n",
      "NLL: 9.0472\n",
      "MSE: 118.2442\n",
      "Validation loss: 8.8900\n",
      "\n",
      "Epoch 33 completed\n",
      "Training loss: 118222.2422\n",
      "NLL: 8.9621\n",
      "MSE: 118.2133\n",
      "Validation loss: 8.7029\n",
      "\n",
      "Epoch 34 completed\n",
      "Training loss: 118267.1297\n",
      "NLL: 8.8314\n",
      "MSE: 118.2583\n",
      "Validation loss: 8.6377\n",
      "\n",
      "Epoch 35 completed\n",
      "Training loss: 118230.3664\n",
      "NLL: 8.7440\n",
      "MSE: 118.2216\n",
      "Validation loss: 8.6002\n",
      "\n",
      "Epoch 36 completed\n",
      "Training loss: 118252.4164\n",
      "NLL: 8.6010\n",
      "MSE: 118.2438\n",
      "Validation loss: 8.5459\n",
      "\n",
      "Epoch 37 completed\n",
      "Training loss: 118193.4062\n",
      "NLL: 8.5621\n",
      "MSE: 118.1848\n",
      "Validation loss: 8.2941\n",
      "\n",
      "Epoch 38 completed\n",
      "Training loss: 118206.1922\n",
      "NLL: 8.4492\n",
      "MSE: 118.1978\n",
      "Validation loss: 8.3096\n",
      "\n",
      "Epoch 39 completed\n",
      "Training loss: 118201.6641\n",
      "NLL: 8.3777\n",
      "MSE: 118.1933\n",
      "Validation loss: 8.2083\n",
      "\n",
      "Epoch 40 completed\n",
      "Training loss: 118197.1016\n",
      "NLL: 8.3615\n",
      "MSE: 118.1887\n",
      "Validation loss: 8.1670\n",
      "\n",
      "Epoch 41 completed\n",
      "Training loss: 118203.5961\n",
      "NLL: 8.2850\n",
      "MSE: 118.1953\n",
      "Validation loss: 8.0595\n",
      "\n",
      "Epoch 42 completed\n",
      "Training loss: 118165.5844\n",
      "NLL: 8.1834\n",
      "MSE: 118.1574\n",
      "Validation loss: 7.9674\n",
      "\n",
      "Epoch 43 completed\n",
      "Training loss: 118169.9352\n",
      "NLL: 8.0734\n",
      "MSE: 118.1619\n",
      "Validation loss: 7.8583\n",
      "\n",
      "Epoch 44 completed\n",
      "Training loss: 118134.1156\n",
      "NLL: 7.9874\n",
      "MSE: 118.1261\n",
      "Validation loss: 7.6748\n",
      "\n",
      "Epoch 45 completed\n",
      "Training loss: 118235.5422\n",
      "NLL: 7.9154\n",
      "MSE: 118.2276\n",
      "Validation loss: 7.7827\n",
      "\n",
      "Epoch 46 completed\n",
      "Training loss: 118204.6125\n",
      "NLL: 7.8675\n",
      "MSE: 118.1967\n",
      "Validation loss: 7.6311\n",
      "\n",
      "Epoch 47 completed\n",
      "Training loss: 118185.9500\n",
      "NLL: 7.8011\n",
      "MSE: 118.1782\n",
      "Validation loss: 7.6347\n",
      "\n",
      "Epoch 48 completed\n",
      "Training loss: 118126.7078\n",
      "NLL: 7.7860\n",
      "MSE: 118.1189\n",
      "Validation loss: 7.7471\n",
      "\n",
      "Epoch 49 completed\n",
      "Training loss: 118118.4820\n",
      "NLL: 7.8086\n",
      "MSE: 118.1107\n",
      "Validation loss: 7.7256\n",
      "\n",
      "Epoch 50 completed\n",
      "Training loss: 118106.3461\n",
      "NLL: 7.7095\n",
      "MSE: 118.0986\n",
      "Validation loss: 7.6074\n",
      "\n",
      "Epoch 51 completed\n",
      "Training loss: 118119.9602\n",
      "NLL: 7.6940\n",
      "MSE: 118.1123\n",
      "Validation loss: 7.5960\n",
      "\n",
      "Epoch 52 completed\n",
      "Training loss: 118122.8891\n",
      "NLL: 7.6987\n",
      "MSE: 118.1152\n",
      "Validation loss: 7.5186\n",
      "\n",
      "Epoch 53 completed\n",
      "Training loss: 118101.3367\n",
      "NLL: 7.6641\n",
      "MSE: 118.0937\n",
      "Validation loss: 7.4014\n",
      "\n",
      "Epoch 54 completed\n",
      "Training loss: 118129.5328\n",
      "NLL: 7.5628\n",
      "MSE: 118.1220\n",
      "Validation loss: 7.3934\n",
      "\n",
      "Epoch 55 completed\n",
      "Training loss: 118086.4461\n",
      "NLL: 7.5570\n",
      "MSE: 118.0789\n",
      "Validation loss: 7.4534\n",
      "\n",
      "Epoch 56 completed\n",
      "Training loss: 118092.0914\n",
      "NLL: 7.5095\n",
      "MSE: 118.0846\n",
      "Validation loss: 7.5015\n",
      "\n",
      "Epoch 57 completed\n",
      "Training loss: 118102.8383\n",
      "NLL: 7.4679\n",
      "MSE: 118.0954\n",
      "Validation loss: 7.4416\n",
      "\n",
      "Epoch 58 completed\n",
      "Training loss: 118124.7133\n",
      "NLL: 7.5434\n",
      "MSE: 118.1172\n",
      "Validation loss: 7.2206\n",
      "\n",
      "Epoch 59 completed\n",
      "Training loss: 118122.3313\n",
      "NLL: 7.5045\n",
      "MSE: 118.1148\n",
      "Validation loss: 7.2491\n",
      "\n",
      "Epoch 60 completed\n",
      "Training loss: 118058.0047\n",
      "NLL: 7.4330\n",
      "MSE: 118.0506\n",
      "Validation loss: 7.3004\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"checkpoint_trainer.py\", line 96, in <module>\n",
      "    loss_info = net.compute_unsupervised_loss(\n",
      "  File \"/home/gsasseville/Files/UDEM/Maitrise/SgrA/SgrA_Interpolation/Tripletformer/tripletformer.py\", line 163, in compute_unsupervised_loss\n",
      "    loglik = self.compute_loglik(mask, px, self.norm)\n",
      "  File \"/home/gsasseville/Files/UDEM/Maitrise/SgrA/SgrA_Interpolation/Tripletformer/tripletformer.py\", line 73, in compute_loglik\n",
      "    log_p = utils.log_normal_pdf(\n",
      "  File \"/home/gsasseville/Files/UDEM/Maitrise/SgrA/SgrA_Interpolation/Tripletformer/utils.py\", line 16, in log_normal_pdf\n",
      "    const = torch.from_numpy(np.array([2.0 * np.pi])).float().to(x.device)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python checkpoint_trainer.py --niters 200 --batch-size 240 --dataset xray --norm --shuffle --sample-tp 0.1 --mse-weight 1000 --imab-dim 64 --cab-dim 256 --decoder-dim 128 --nlayers 1 --sample-type random --num-ref-points 128 #--experiment-id 1010981"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_prediction.py --niters 1 --batch-size 240 --dataset xray --norm --shuffle --sample-tp 0.1 --mse-weight 0.01 --imab-dim 64 --cab-dim 256 --decoder-dim 128 --nlayers 1 --sample-type random --num-ref-points 128 --experiment-id 7935467\n",
      "(2400, 960, 3) (1280, 960, 3) (320, 960, 3)\n",
      "test_prediction.py:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  chp = torch.load(f'./saved_models/{args.dataset}_{args.experiment_id}.h5')\n",
      "Number of points to predict:  tensor(192, device='cuda:0')\n",
      "Length of means 192\n",
      "Length of channel indices 192\n",
      "Interpolation of Channel:  0\n",
      "Amount of points to predict:  192 for channel:  0\n",
      "Length of means:  192\n",
      "Most frequent value: 6.8802085\n",
      "Count: 9\n",
      "Mean of the means:  7.0421515\n",
      "Time indices of artefacts:  [921 924 932 939 941 943 955 957 959]\n",
      "Length of this array of indices:  9\n",
      "Total pred points is: 192\n"
     ]
    }
   ],
   "source": [
    "!python test_prediction.py --niters 1 --batch-size 240 --dataset xray --norm --shuffle --sample-tp 0.1 --mse-weight 0.01 --imab-dim 64 --cab-dim 256 --decoder-dim 128 --nlayers 1 --sample-type random --num-ref-points 128 --experiment-id 7935467"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tripletformer-nUF7tw2u",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
