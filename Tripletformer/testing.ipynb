{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint_trainer.py --niters 300 --batch-size 60 --dataset randomwalk --norm --shuffle --sample-tp 0.1 --mse-weight 1.0 --imab-dim 64 --cab-dim 256 --decoder-dim 128 --nlayers 1 --sample-type random --num-ref-points 128 --experiment-id 8938992\n",
      "(50000, 200, 9) (25000, 200, 9) (25000, 200, 9)\n",
      "/home/gsasseville/.local/share/virtualenvs/Tripletformer-nUF7tw2u/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "checkpoint_trainer.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "Resuming training from checkpoint at Epoch 51\n",
      "Experiment ID 8938992\n",
      "Epoch 51 completed\n",
      "Training loss: 1.9918\n",
      "Validation loss: 2.0484\n",
      "Epoch 52 completed\n",
      "Training loss: 1.9679\n",
      "Validation loss: 1.9475\n",
      "Epoch 53 completed\n",
      "Training loss: 1.9659\n",
      "Validation loss: 1.9399\n",
      "Epoch 54 completed\n",
      "Training loss: 1.9710\n",
      "Validation loss: 1.9705\n",
      "Epoch 55 completed\n",
      "Training loss: 1.9650\n",
      "Validation loss: 1.9363\n",
      "Epoch 56 completed\n",
      "Training loss: 1.9654\n",
      "Validation loss: 1.9564\n",
      "Epoch 57 completed\n",
      "Training loss: 1.9552\n",
      "Validation loss: 1.9431\n",
      "Epoch 58 completed\n",
      "Training loss: 1.9533\n",
      "Validation loss: 1.9857\n",
      "Epoch 59 completed\n",
      "Training loss: 1.9523\n",
      "Validation loss: 1.9254\n",
      "Epoch 60 completed\n",
      "Training loss: 1.9487\n",
      "Validation loss: 1.9627\n",
      "Epoch 61 completed\n",
      "Training loss: 1.9403\n",
      "Validation loss: 1.9095\n",
      "Epoch 62 completed\n",
      "Training loss: 1.9426\n",
      "Validation loss: 1.9192\n",
      "Epoch 63 completed\n",
      "Training loss: 1.9404\n",
      "Validation loss: 2.0072\n",
      "Epoch 64 completed\n",
      "Training loss: 1.9318\n",
      "Validation loss: 1.9680\n",
      "Epoch 65 completed\n",
      "Training loss: 1.9391\n",
      "Validation loss: 1.9338\n",
      "Epoch 66 completed\n",
      "Training loss: 1.9371\n",
      "Validation loss: 1.9261\n",
      "Epoch 67 completed\n",
      "Training loss: 1.9233\n",
      "Validation loss: 1.9342\n",
      "Epoch 68 completed\n",
      "Training loss: 1.9377\n",
      "Validation loss: 1.9138\n",
      "Epoch 69 completed\n",
      "Training loss: 1.9232\n",
      "Validation loss: 1.9137\n",
      "Epoch 70 completed\n",
      "Training loss: 1.9211\n",
      "Validation loss: 1.9376\n",
      "Epoch 71 completed\n",
      "Training loss: 1.9262\n",
      "Validation loss: 1.9721\n",
      "Epoch 72 completed\n",
      "Training loss: 1.9369\n",
      "Validation loss: 1.9570\n",
      "Epoch 73 completed\n",
      "Training loss: 1.8941\n",
      "Validation loss: 1.8782\n",
      "Epoch 74 completed\n",
      "Training loss: 1.8867\n",
      "Validation loss: 1.8913\n",
      "Epoch 75 completed\n",
      "Training loss: 1.8905\n",
      "Validation loss: 1.8759\n",
      "Epoch 76 completed\n",
      "Training loss: 1.8784\n",
      "Validation loss: 1.8823\n",
      "Epoch 77 completed\n",
      "Training loss: 1.8806\n",
      "Validation loss: 1.8717\n",
      "Epoch 78 completed\n",
      "Training loss: 1.8855\n",
      "Validation loss: 1.8699\n",
      "Epoch 79 completed\n",
      "Training loss: 1.8755\n",
      "Validation loss: 1.8593\n",
      "Epoch 80 completed\n",
      "Training loss: 1.8746\n",
      "Validation loss: 1.8722\n",
      "Epoch 81 completed\n",
      "Training loss: 1.8721\n",
      "Validation loss: 1.8528\n",
      "Epoch 82 completed\n",
      "Training loss: 1.8732\n",
      "Validation loss: 1.8792\n",
      "Epoch 83 completed\n",
      "Training loss: 1.8613\n",
      "Validation loss: 1.8584\n",
      "Epoch 84 completed\n",
      "Training loss: 1.8617\n",
      "Validation loss: 1.9259\n",
      "Epoch 85 completed\n",
      "Training loss: 1.8562\n",
      "Validation loss: 1.8666\n",
      "Epoch 86 completed\n",
      "Training loss: 1.8571\n",
      "Validation loss: 1.8561\n",
      "Epoch 87 completed\n",
      "Training loss: 1.8553\n",
      "Validation loss: 1.8457\n",
      "Epoch 88 completed\n",
      "Training loss: 1.8418\n",
      "Validation loss: 1.8632\n",
      "Epoch 89 completed\n",
      "Training loss: 1.8418\n",
      "Validation loss: 1.8937\n",
      "Epoch 90 completed\n",
      "Training loss: 1.8311\n",
      "Validation loss: 1.8234\n",
      "Epoch 91 completed\n",
      "Training loss: 1.8231\n",
      "Validation loss: 1.8122\n",
      "Epoch 92 completed\n",
      "Training loss: 1.8324\n",
      "Validation loss: 1.8259\n",
      "Epoch 93 completed\n",
      "Training loss: 1.8225\n",
      "Validation loss: 1.8048\n",
      "Epoch 94 completed\n",
      "Training loss: 1.8140\n",
      "Validation loss: 1.7840\n",
      "Epoch 95 completed\n",
      "Training loss: 1.8125\n",
      "Validation loss: 1.8157\n",
      "Epoch 96 completed\n",
      "Training loss: 1.8050\n",
      "Validation loss: 1.8311\n",
      "Epoch 97 completed\n",
      "Training loss: 1.8040\n",
      "Validation loss: 1.8283\n",
      "Epoch 98 completed\n",
      "Training loss: 1.7932\n",
      "Validation loss: 1.7756\n",
      "Epoch 99 completed\n",
      "Training loss: 1.7964\n",
      "Validation loss: 1.7661\n",
      "Epoch 100 completed\n",
      "Training loss: 1.7904\n",
      "Validation loss: 1.8252\n",
      "Epoch 101 completed\n",
      "Training loss: 1.7976\n",
      "Validation loss: 1.7690\n",
      "Epoch 102 completed\n",
      "Training loss: 1.7830\n",
      "Validation loss: 1.8014\n",
      "Epoch 103 completed\n",
      "Training loss: 1.7798\n",
      "Validation loss: 1.7911\n",
      "Epoch 104 completed\n",
      "Training loss: 1.7777\n",
      "Validation loss: 1.7732\n",
      "Epoch 105 completed\n",
      "Training loss: 1.7702\n",
      "Validation loss: 1.8031\n",
      "Epoch 106 completed\n",
      "Training loss: 1.7734\n",
      "Validation loss: 1.7517\n",
      "Epoch 107 completed\n",
      "Training loss: 1.7691\n",
      "Validation loss: 1.7847\n",
      "Epoch 108 completed\n",
      "Training loss: 1.7677\n",
      "Validation loss: 1.7916\n",
      "Epoch 109 completed\n",
      "Training loss: 1.7673\n",
      "Validation loss: 1.7421\n",
      "Epoch 110 completed\n",
      "Training loss: 1.7710\n",
      "Validation loss: 1.7802\n",
      "Epoch 111 completed\n",
      "Training loss: 1.7594\n",
      "Validation loss: 1.7647\n",
      "Epoch 112 completed\n",
      "Training loss: 1.7576\n",
      "Validation loss: 1.7480\n",
      "Epoch 113 completed\n",
      "Training loss: 1.7553\n",
      "Validation loss: 1.7493\n",
      "Epoch 114 completed\n",
      "Training loss: 1.7545\n",
      "Validation loss: 1.8031\n",
      "Epoch 115 completed\n",
      "Training loss: 1.7476\n",
      "Validation loss: 1.7378\n",
      "Epoch 116 completed\n",
      "Training loss: 1.7479\n",
      "Validation loss: 1.7461\n",
      "Epoch 117 completed\n",
      "Training loss: 1.7438\n",
      "Validation loss: 1.7504\n",
      "Epoch 118 completed\n",
      "Training loss: 1.7481\n",
      "Validation loss: 1.7232\n"
     ]
    }
   ],
   "source": [
    "!python checkpoint_trainer.py --niters 300 --batch-size 60 --dataset randomwalk --norm --shuffle --sample-tp 0.1 --mse-weight 1.0 --imab-dim 64 --cab-dim 256 --decoder-dim 128 --nlayers 1 --sample-type random --num-ref-points 128 --experiment-id 8938992"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_prediction.py --batch-size 60 --dataset randomwalk --norm --shuffle --sample-tp 0.1 --mse-weight 1.0 --imab-dim 64 --cab-dim 256 --decoder-dim 128 --nlayers 1 --sample-type random --num-ref-points 128 --experiment-id 8938992\n",
      "(50000, 200, 9) (25000, 200, 9) (25000, 200, 9)\n",
      "test_prediction.py:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  chp = torch.load(f'./saved_models/{args.dataset}_{args.experiment_id}.h5')\n",
      "Number of points to predict:  tensor(332, device='cuda:0')\n",
      "Number of points to predict:  tensor(301, device='cuda:0')\n",
      "Number of points to predict:  tensor(318, device='cuda:0')\n",
      "Number of points to predict:  tensor(337, device='cuda:0')\n",
      "Number of points to predict:  tensor(302, device='cuda:0')\n",
      "Number of points to predict:  tensor(323, device='cuda:0')\n",
      "Number of points to predict:  tensor(327, device='cuda:0')\n",
      "Number of points to predict:  tensor(307, device='cuda:0')\n",
      "Number of points to predict:  tensor(303, device='cuda:0')\n",
      "Number of points to predict:  tensor(320, device='cuda:0')\n",
      "Number of points to predict:  tensor(352, device='cuda:0')\n",
      "Number of points to predict:  tensor(320, device='cuda:0')\n",
      "Number of points to predict:  tensor(361, device='cuda:0')\n",
      "Number of points to predict:  tensor(341, device='cuda:0')\n",
      "Number of points to predict:  tensor(315, device='cuda:0')\n",
      "Number of points to predict:  tensor(315, device='cuda:0')\n",
      "Number of points to predict:  tensor(324, device='cuda:0')\n",
      "Number of points to predict:  tensor(333, device='cuda:0')\n",
      "Number of points to predict:  tensor(332, device='cuda:0')\n",
      "Number of points to predict:  tensor(334, device='cuda:0')\n",
      "Number of points to predict:  tensor(307, device='cuda:0')\n",
      "Number of points to predict:  tensor(318, device='cuda:0')\n",
      "Number of points to predict:  tensor(318, device='cuda:0')\n",
      "Number of points to predict:  tensor(320, device='cuda:0')\n",
      "Number of points to predict:  tensor(305, device='cuda:0')\n",
      "Number of points to predict:  tensor(313, device='cuda:0')\n",
      "Number of points to predict:  tensor(307, device='cuda:0')\n",
      "Number of points to predict:  tensor(325, device='cuda:0')\n",
      "Number of points to predict:  tensor(294, device='cuda:0')\n",
      "Number of points to predict:  tensor(316, device='cuda:0')\n",
      "Number of points to predict:  tensor(305, device='cuda:0')\n",
      "Number of points to predict:  tensor(316, device='cuda:0')\n",
      "Number of points to predict:  tensor(316, device='cuda:0')\n",
      "Number of points to predict:  tensor(331, device='cuda:0')\n",
      "Number of points to predict:  tensor(304, device='cuda:0')\n",
      "Number of points to predict:  tensor(350, device='cuda:0')\n",
      "Number of points to predict:  tensor(316, device='cuda:0')\n",
      "Number of points to predict:  tensor(338, device='cuda:0')\n",
      "Number of points to predict:  tensor(337, device='cuda:0')\n",
      "Number of points to predict:  tensor(335, device='cuda:0')\n",
      "Number of points to predict:  tensor(323, device='cuda:0')\n",
      "Number of points to predict:  tensor(318, device='cuda:0')\n",
      "Number of points to predict:  tensor(340, device='cuda:0')\n",
      "Number of points to predict:  tensor(310, device='cuda:0')\n",
      "Number of points to predict:  tensor(321, device='cuda:0')\n",
      "Number of points to predict:  tensor(331, device='cuda:0')\n",
      "Number of points to predict:  tensor(327, device='cuda:0')\n",
      "Number of points to predict:  tensor(311, device='cuda:0')\n",
      "Number of points to predict:  tensor(314, device='cuda:0')\n",
      "Number of points to predict:  tensor(332, device='cuda:0')\n",
      "Number of points to predict:  tensor(322, device='cuda:0')\n",
      "Length of means 322\n",
      "Length of channel indices 322\n",
      "Interpolation of Channel:  0\n",
      "Amount of points to predict:  81 for channel:  0\n",
      "Length of means:  81\n",
      "Most frequent value: -14.268075\n",
      "Count: 1\n",
      "Mean of the means:  1.2322121\n",
      "Time indices of artefacts:  [193]\n",
      "Length of this array of indices:  1\n",
      "Interpolation of Channel:  1\n",
      "Amount of points to predict:  85 for channel:  1\n",
      "Length of means:  85\n",
      "Most frequent value: -12.118792\n",
      "Count: 1\n",
      "Mean of the means:  1.8589002\n",
      "Time indices of artefacts:  [48]\n",
      "Length of this array of indices:  1\n",
      "Interpolation of Channel:  2\n",
      "Amount of points to predict:  79 for channel:  2\n",
      "Length of means:  79\n",
      "Most frequent value: -39.42002\n",
      "Count: 1\n",
      "Mean of the means:  -24.002771\n",
      "Time indices of artefacts:  [187]\n",
      "Length of this array of indices:  1\n",
      "Interpolation of Channel:  3\n",
      "Amount of points to predict:  77 for channel:  3\n",
      "Length of means:  77\n",
      "Most frequent value: -5.137623\n",
      "Count: 1\n",
      "Mean of the means:  4.5812182\n",
      "Time indices of artefacts:  [35]\n",
      "Length of this array of indices:  1\n",
      "Total pred points is: 322\n"
     ]
    }
   ],
   "source": [
    "!python test_prediction.py --batch-size 60 --dataset randomwalk --norm --shuffle --sample-tp 0.1 --mse-weight 1.0 --imab-dim 64 --cab-dim 256 --decoder-dim 128 --nlayers 1 --sample-type random --num-ref-points 128 --experiment-id 8938992"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python checkpoint_trainer.py --niters 2000 --batch-size 20 --dataset sgra --norm --shuffle --sample-tp 0.1 --mse-weight 1.0 --imab-dim 64 --cab-dim 256 --decoder-dim 128 --nlayers 1 --sample-type random --num-ref-points 128 --experiment-id 982081"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_prediction.py --niters 1 --batch-size 20 --dataset sgrA --norm --shuffle --sample-tp 0.1 --mse-weight 1.0 --imab-dim 64 --cab-dim 256 --decoder-dim 128 --nlayers 1 --sample-type random --num-ref-points 128 --experiment-id 982081\n",
      "(6300, 960, 9) (1260, 960, 9) (840, 960, 9)\n",
      "test_prediction.py:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  chp = torch.load(f'./saved_models/{args.dataset}_{args.experiment_id}.h5')\n",
      "Number of points to predict:  tensor(1967, device='cuda:0')\n",
      "Length of means 1967\n",
      "Length of channel indices 1967\n",
      "Interpolation of Channel:  0\n",
      "Amount of points to predict:  771 for channel:  0\n",
      "Length of means:  771\n",
      "Most frequent value: -0.077152\n",
      "Count: 156\n",
      "Mean of the means:  -0.15843123\n",
      "Time indices of artefacts:  [  0  25  27  28  44  46  52  53  57  64  65  71  73  74  76  79  88  90\n",
      " 152 163 165 175 179 182 188 189 191 208 210 212 230 249 250 254 266 273\n",
      " 276 277 303 305 311 328 337 341 343 358 365 390 392 419 434 435 437 441\n",
      " 447 460 472 477 486 504 505 506 513 525 535 536 550 567 568 574 578 581\n",
      " 582 583 587 592 598 604 608 610 617 619 630 640 654 655 666 668 671 673\n",
      " 674 682 687 693 694 695 696 700 701 708 709 722 730 735 737 741 742 743\n",
      " 747 750 751 754 757 759 760 763 764 766 770 780 786 787 791 794 802 809\n",
      " 816 821 822 824 825 837 841 842 848 853 854 856 858 871 878 879 885 897\n",
      " 906 911 916 919 922 928 933 942 946 951 952 953]\n",
      "Length of this array of indices:  156\n",
      "Interpolation of Channel:  1\n",
      "Amount of points to predict:  192 for channel:  1\n",
      "Length of means:  192\n",
      "Most frequent value: -0.19876918\n",
      "Count: 53\n",
      "Mean of the means:  -0.038088035\n",
      "Time indices of artefacts:  [ 27  28  44  57  63  71  88 105 115 152 165 171 183 188 201 241 250 255\n",
      " 266 277 296 358 365 390 429 430 477 504 550 598 604 630 640 666 674 695\n",
      " 701 730 735 737 742 743 750 751 766 791 802 821 841 853 854 875 922]\n",
      "Length of this array of indices:  53\n",
      "Interpolation of Channel:  2\n",
      "Amount of points to predict:  812 for channel:  2\n",
      "Length of means:  812\n",
      "Most frequent value: -0.38460296\n",
      "Count: 180\n",
      "Mean of the means:  -0.39924195\n",
      "Time indices of artefacts:  [  0  25  27  28  44  46  52  53  57  64  65  71  73  74  76  79  88  90\n",
      "  92 147 148 165 171 172 182 188 189 191 198 201 208 210 212 215 236 238\n",
      " 241 249 250 254 273 276 277 291 296 298 303 305 307 310 311 314 322 325\n",
      " 328 332 341 343 365 367 379 390 392 404 419 434 435 437 441 444 447 460\n",
      " 462 472 496 505 506 513 514 535 536 541 550 567 568 578 581 582 583 587\n",
      " 592 598 604 608 610 617 619 630 640 654 655 666 668 671 673 674 682 687\n",
      " 693 694 695 696 700 701 708 709 722 730 735 737 741 742 743 747 750 751\n",
      " 754 757 759 760 763 764 766 770 777 778 780 786 787 791 794 802 806 807\n",
      " 808 809 816 821 822 824 825 827 828 829 837 841 842 848 853 854 856 858\n",
      " 871 878 879 885 897 906 910 911 916 919 922 928 933 942 946 951 952 953]\n",
      "Length of this array of indices:  180\n",
      "Interpolation of Channel:  3\n",
      "Amount of points to predict:  192 for channel:  3\n",
      "Length of means:  192\n",
      "Most frequent value: -0.20903042\n",
      "Count: 192\n",
      "/home/gsasseville/.local/share/virtualenvs/Tripletformer-nUF7tw2u/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/gsasseville/.local/share/virtualenvs/Tripletformer-nUF7tw2u/lib/python3.8/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Mean of the means:  nan\n",
      "Time indices of artefacts:  [ 25  27  28  44  46  52  53  57  64  65  71  73  74  76  79  88  90  92\n",
      " 105 115 129 147 148 152 163 165 171 172 175 182 188 189 191 198 201 208\n",
      " 210 212 215 230 236 238 241 242 249 250 254 255 258 266 273 276 277 284\n",
      " 291 296 298 303 305 310 311 314 322 325 328 332 335 337 341 343 358 365\n",
      " 367 379 388 390 392 404 419 434 435 437 441 444 447 460 462 472 477 486\n",
      " 496 499 504 505 506 511 513 514 523 525 535 536 541 550 567 568 578 581\n",
      " 582 583 587 592 598 604 608 610 617 619 630 640 654 655 666 668 671 673\n",
      " 674 682 687 693 694 695 696 700 701 708 709 722 730 735 737 741 742 743\n",
      " 747 750 751 754 757 759 760 763 764 766 770 780 786 787 791 794 802 809\n",
      " 816 821 822 824 825 837 841 842 848 853 854 856 858 871 878 879 885 897\n",
      " 906 911 916 919 922 928 933 942 946 951 952 953]\n",
      "Length of this array of indices:  192\n",
      "Total pred points is: 1967\n"
     ]
    }
   ],
   "source": [
    "!python test_prediction.py --niters 1 --batch-size 20 --dataset sgrA --norm --shuffle --sample-tp 0.1 --mse-weight 1.0 --imab-dim 64 --cab-dim 256 --decoder-dim 128 --nlayers 1 --sample-type random --num-ref-points 128 --experiment-id 982081"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing elements in array 1: [  0  39 167 610 642 889]\n",
      "All elements of array 2 are found in the main array.\n",
      "Missing elements in array 3: [  0  34 426 950]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Main array of integers\n",
    "main_array = np.array([\n",
    "    1, 3, 5, 17, 23, 25, 26, 28, 30, 37, 43, 47, 48, 67, 69, 75, 82, 83,\n",
    "    92, 94, 98, 100, 101, 113, 119, 121, 123, 124, 127, 131, 134, 136,\n",
    "    142, 150, 154, 156, 161, 162, 165, 177, 180, 186, 197, 199, 206, 209,\n",
    "    212, 215, 216, 217, 221, 222, 226, 231, 233, 261, 287, 293, 297, 302,\n",
    "    303, 309, 311, 318, 319, 324, 326, 327, 332, 344, 347, 357, 378, 381,\n",
    "    382, 383, 385, 387, 388, 390, 391, 393, 398, 400, 407, 416, 422, 429,\n",
    "    431, 437, 441, 445, 455, 466, 470, 472, 488, 494, 500, 503, 513, 515,\n",
    "    520, 524, 525, 527, 531, 533, 534, 537, 548, 553, 567, 574, 576, 577,\n",
    "    580, 582, 586, 588, 598, 602, 603, 604, 616, 624, 626, 627, 628, 633,\n",
    "    643, 651, 657, 666, 680, 711, 712, 713, 721, 727, 728, 729, 730, 731,\n",
    "    733, 735, 738, 747, 757, 758, 767, 776, 777, 778, 782, 784, 789, 803,\n",
    "    806, 807, 809, 810, 812, 817, 829, 833, 835, 838, 839, 859, 861, 868,\n",
    "    870, 872, 883, 884, 885, 887, 892, 895, 900, 901, 903, 904, 907, 908,\n",
    "    910, 911, 925, 937, 940, 951\n",
    "])\n",
    "\n",
    "# Arrays to check\n",
    "arrays_to_check = [\n",
    "    np.array([\n",
    "        0, 1, 3, 5, 17, 23, 25, 26, 28, 30, 37, 39, 43, 47, 48, 67, 69, 75,\n",
    "        82, 83, 92, 94, 98, 100, 101, 113, 119, 121, 123, 124, 127, 131, 134,\n",
    "        136, 142, 150, 154, 156, 161, 162, 165, 167, 177, 180, 186, 197, 199,\n",
    "        206, 209, 212, 215, 216, 217, 221, 222, 226, 231, 233, 261, 287, 293,\n",
    "        297, 302, 303, 309, 311, 318, 319, 324, 326, 327, 347, 357, 378, 387,\n",
    "        388, 390, 391, 400, 416, 422, 429, 431, 437, 441, 445, 455, 466, 470,\n",
    "        494, 503, 515, 524, 525, 527, 531, 533, 534, 537, 548, 553, 567, 574,\n",
    "        586, 588, 598, 603, 610, 627, 633, 642, 643, 651, 657, 666, 680, 711,\n",
    "        712, 713, 727, 735, 738, 747, 757, 758, 767, 776, 777, 778, 782, 784,\n",
    "        789, 803, 806, 807, 809, 810, 812, 817, 829, 833, 835, 838, 839, 859,\n",
    "        861, 868, 870, 872, 883, 884, 885, 887, 889, 892, 895, 900, 901, 903,\n",
    "        904, 907, 908, 910, 911, 925, 937, 940, 951\n",
    "    ]),\n",
    "    np.array([\n",
    "        5, 25, 98, 119, 123, 127, 131, 206, 209, 261, 287, 318, 319, 327,\n",
    "        382, 400, 466, 470, 494, 525, 527, 537, 576, 577, 580, 586, 588,\n",
    "        604, 666, 757, 767, 784, 838, 859, 895, 937\n",
    "    ]),\n",
    "    np.array([\n",
    "        0, 1, 3, 5, 17, 23, 25, 26, 28, 30, 34, 37, 43, 47, 48, 67, 69, 75,\n",
    "        82, 83, 92, 94, 98, 100, 101, 113, 119, 121, 123, 124, 127, 131, 134,\n",
    "        136, 142, 150, 154, 156, 161, 162, 165, 177, 180, 186, 197, 199, 206,\n",
    "        209, 212, 215, 216, 217, 221, 222, 226, 231, 233, 261, 287, 293, 297,\n",
    "        302, 303, 309, 311, 318, 319, 324, 326, 327, 332, 344, 357, 381, 382,\n",
    "        383, 385, 387, 391, 393, 398, 407, 416, 426, 429, 431, 437, 441, 455,\n",
    "        466, 470, 472, 488, 494, 503, 513, 515, 520, 524, 525, 527, 534, 548,\n",
    "        580, 586, 588, 602, 603, 604, 616, 624, 628, 633, 657, 711, 712, 713,\n",
    "        721, 727, 728, 730, 731, 738, 757, 767, 776, 778, 782, 784, 789, 803,\n",
    "        806, 807, 809, 810, 812, 817, 829, 833, 835, 838, 839, 859, 861, 868,\n",
    "        870, 872, 883, 884, 885, 887, 892, 895, 900, 901, 903, 904, 907, 908,\n",
    "        910, 911, 925, 937, 940, 950, 951\n",
    "    ])\n",
    "]\n",
    "\n",
    "# Check each array and find missing elements\n",
    "for idx, array in enumerate(arrays_to_check):\n",
    "    missing_elements = array[~np.isin(array, main_array)]\n",
    "    if len(missing_elements) > 0:\n",
    "        print(f\"Missing elements in array {idx + 1}: {missing_elements}\")\n",
    "    else:\n",
    "        print(f\"All elements of array {idx + 1} are found in the main array.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint_trainer.py --niters 300 --batch-size 240 --dataset xray --norm --shuffle --sample-tp 0.1 --mse-weight 1.0 --imab-dim 64 --cab-dim 256 --decoder-dim 128 --nlayers 1 --sample-type random --num-ref-points 128 --experiment-id 4424066\n",
      "(2400, 960, 3) (1280, 960, 3) (320, 960, 3)\n",
      "/home/gsasseville/.local/share/virtualenvs/Tripletformer-nUF7tw2u/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "checkpoint_trainer.py:70: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "Resuming training from checkpoint at Epoch 12\n",
      "Experiment ID 4424066\n",
      "Epoch 12 completed\n",
      "Training loss: 4.3665\n",
      "Validation loss: 4.3632\n",
      "Epoch 13 completed\n",
      "Training loss: 4.4898\n",
      "Validation loss: 4.3181\n",
      "Epoch 14 completed\n",
      "Training loss: 4.3045\n",
      "Validation loss: 4.2206\n",
      "Epoch 15 completed\n",
      "Training loss: 4.2929\n",
      "Validation loss: 4.2532\n",
      "Epoch 16 completed\n",
      "Training loss: 4.2757\n",
      "Validation loss: 4.1559\n",
      "Epoch 17 completed\n",
      "Training loss: 4.1952\n",
      "Validation loss: 4.2706\n",
      "Epoch 18 completed\n",
      "Training loss: 4.1973\n",
      "Validation loss: 4.0857\n",
      "Epoch 19 completed\n",
      "Training loss: 4.0886\n",
      "Validation loss: 4.0633\n",
      "Epoch 20 completed\n",
      "Training loss: 4.0733\n",
      "Validation loss: 4.0215\n",
      "Epoch 21 completed\n",
      "Training loss: 4.0288\n",
      "Validation loss: 3.9852\n",
      "Epoch 22 completed\n",
      "Training loss: 4.0049\n",
      "Validation loss: 3.9364\n",
      "Epoch 23 completed\n",
      "Training loss: 3.9481\n",
      "Validation loss: 3.9220\n",
      "Epoch 24 completed\n",
      "Training loss: 3.9347\n",
      "Validation loss: 3.8679\n",
      "Epoch 25 completed\n",
      "Training loss: 3.8844\n",
      "Validation loss: 3.8512\n",
      "Epoch 26 completed\n",
      "Training loss: 3.8778\n",
      "Validation loss: 3.8399\n",
      "Epoch 27 completed\n",
      "Training loss: 3.8393\n",
      "Validation loss: 3.8601\n",
      "Epoch 28 completed\n",
      "Training loss: 3.9061\n",
      "Validation loss: 3.8162\n",
      "Epoch 29 completed\n",
      "Training loss: 3.8545\n",
      "Validation loss: 3.8207\n",
      "Epoch 30 completed\n",
      "Training loss: 3.8428\n",
      "Validation loss: 3.8002\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"checkpoint_trainer.py\", line 90, in <module>\n",
      "    original_mask = torch.ones(train_batch[:, :, dim:2 * dim].shape).to(device)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python checkpoint_trainer.py --niters 200 --batch-size 240 --dataset xray --norm --shuffle --sample-tp 0.1 --mse-weight 1.0 --imab-dim 64 --cab-dim 256 --decoder-dim 128 --nlayers 1 --sample-type random --num-ref-points 128 --experiment-id 4424066"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_prediction.py --niters 1 --batch-size 240 --dataset xray --norm --shuffle --sample-tp 0.1 --mse-weight 1.0 --imab-dim 64 --cab-dim 256 --decoder-dim 128 --nlayers 1 --sample-type random --num-ref-points 128 --experiment-id 4424066\n",
      "(2400, 960, 3) (1280, 960, 3) (320, 960, 3)\n",
      "test_prediction.py:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  chp = torch.load(f'./saved_models/{args.dataset}_{args.experiment_id}.h5')\n",
      "Number of points to predict:  tensor(192, device='cuda:0')\n",
      "Length of means 192\n",
      "Length of channel indices 192\n",
      "Interpolation of Channel:  0\n",
      "Amount of points to predict:  192 for channel:  0\n",
      "Length of means:  192\n",
      "Most frequent value: 6.311365\n",
      "Count: 13\n",
      "Mean of the means:  6.4143515\n",
      "Time indices of artefacts:  [ 2  7  8 11 13 16 18 23 27 29 31 48 58]\n",
      "Length of this array of indices:  13\n",
      "Total pred points is: 192\n"
     ]
    }
   ],
   "source": [
    "!python test_prediction.py --niters 1 --batch-size 240 --dataset xray --norm --shuffle --sample-tp 0.1 --mse-weight 1.0 --imab-dim 64 --cab-dim 256 --decoder-dim 128 --nlayers 1 --sample-type random --num-ref-points 128 --experiment-id 4424066"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tripletformer-nUF7tw2u",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
