{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9471/4001594433.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model path: save/sgra_fold0_14_20250403_134958/best_model.pth\n",
      "Model losses:\n",
      "  sgra_fold0_13_20250403_135329: 45.67409662902355\n",
      "  sgra_fold0_5_20250403_131713: 46.16662035882473\n",
      "  sgra_fold0_11_20250403_134231: 72.1814873367548\n",
      "  sgra_fold0_8_20250403_132651: 47.306783854961395\n",
      "  sgra_fold0_1_20250403_131714: 47.64918717741966\n",
      "  sgra_fold0_3_20250403_131713: 72.03574284911156\n",
      "  sgra_fold0_6_20250403_133742: 46.412629410624504\n",
      "  sgra_fold0_4_20250403_131714: 59.40389084815979\n",
      "  sgra_fold0_14_20250403_134958: 45.346777737140656\n",
      "  sgra_fold0_9_20250403_132649: 48.37328949570656\n",
      "  sgra_fold0_7_20250403_132025: 47.41524352133274\n",
      "  sgra_fold0_2_20250403_131714: 47.98417070508003\n",
      "  sgra_fold0_15_20250403_160151: 65.59162385761738\n",
      "  sgra_fold0_16_20250403_160255: 64.0322804003954\n",
      "  sgra_fold0_12_20250403_135004: 68.89293368160725\n",
      "  sgra_fold0_10_20250403_134232: 48.88146764039993\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import re\n",
    "\n",
    "def find_best_model_and_losses(directory=\"save\"):\n",
    "    \"\"\"\n",
    "    Finds the best model among 16 trained models based on their validation losses.\n",
    "\n",
    "    Args:\n",
    "        directory (str): The directory containing the model folders.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the best model's path and a dictionary of all model losses.\n",
    "    \"\"\"\n",
    "\n",
    "    model_losses = {}\n",
    "    best_model_path = None\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    # Regular expression to match folder names\n",
    "    folder_pattern = re.compile(r\"sgra_fold0_\\d+_\\d{8}_\\d{6}\")\n",
    "\n",
    "    # Iterate through all subdirectories in the given directory\n",
    "    for folder_name in os.listdir(directory):\n",
    "        folder_path = os.path.join(directory, folder_name)\n",
    "\n",
    "        # Check if the path is a directory and matches the folder name pattern\n",
    "        if os.path.isdir(folder_path) and folder_pattern.match(folder_name):\n",
    "            model_file_path = os.path.join(folder_path, \"best_model.pth\")\n",
    "\n",
    "            # Check if the model file exists\n",
    "            if os.path.exists(model_file_path):\n",
    "                try:\n",
    "                    checkpoint = torch.load(model_file_path)\n",
    "                    loss = checkpoint['loss']\n",
    "                    model_losses[folder_name] = loss\n",
    "\n",
    "                    # Update best model if current loss is lower\n",
    "                    if loss < best_loss:\n",
    "                        best_loss = loss\n",
    "                        best_model_path = model_file_path\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading model from {model_file_path}: {e}\")\n",
    "\n",
    "    return best_model_path, model_losses\n",
    "\n",
    "def print_best_model_and_losses(directory=\"save\"):\n",
    "    \"\"\"\n",
    "    Prints the best model path and all model losses.\n",
    "\n",
    "    Args:\n",
    "        directory (str): The directory containing the model folders.\n",
    "    \"\"\"\n",
    "    best_model_path, model_losses = find_best_model_and_losses(directory)\n",
    "\n",
    "    if best_model_path:\n",
    "        print(f\"Best model path: {best_model_path}\")\n",
    "        print(\"Model losses:\")\n",
    "        for model_name, loss in model_losses.items():\n",
    "            print(f\"  {model_name}: {loss}\")\n",
    "    else:\n",
    "        print(\"No valid models found.\")\n",
    "\n",
    "# Example usage:\n",
    "print_best_model_and_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(config='base.yaml', device='cuda:0', seed=1, testmissingratio=0.1, nfold=0, unconditional=False, gp_noise=True, modelfolder='sgra_fold0_final_20250404_110307', nsample=100)\n",
      "{\n",
      "    \"train\": {\n",
      "        \"epochs\": 1,\n",
      "        \"batch_size\": 16,\n",
      "        \"lr\": 0.001\n",
      "    },\n",
      "    \"diffusion\": {\n",
      "        \"layers\": 8,\n",
      "        \"channels\": 128,\n",
      "        \"nheads\": 8,\n",
      "        \"diffusion_embedding_dim\": 256,\n",
      "        \"beta_start\": 0.0001,\n",
      "        \"beta_end\": 0.5,\n",
      "        \"num_steps\": 50,\n",
      "        \"schedule\": \"quad\"\n",
      "    },\n",
      "    \"model\": {\n",
      "        \"is_unconditional\": false,\n",
      "        \"timeemb\": 128,\n",
      "        \"featureemb\": 16,\n",
      "        \"target_strategy\": \"random\",\n",
      "        \"test_missing_ratio\": 0.1\n",
      "    }\n",
      "}\n",
      "/home/gsasseville/.local/share/virtualenvs/Diffusion-Nf9kVZlp/lib/python3.11/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "Loading pre-trained model from folder: ./save/sgra_fold0_final_20250404_110307/model.pth\n",
      "/home/gsasseville/Files/UDEM/Maitrise/SgrA/SgrA_Interpolation/Diffusion/exe_sgra.py:66: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"./save/\" + args.modelfolder + \"/model.pth\")\n",
      "Evaluating Model\n",
      "eval_mask[:, feature_idx].bool() tensor([False, False, False,  ..., False, False, False])\n",
      "predicted_values[eval_mask[:, feature_idx].bool(), feature_idx].numpy() [-1.1329057  -1.1340879  -0.15247424  0.8308988  -0.14965835 -0.15176082\n",
      " -0.15173784 -1.1290559  -1.1306194  -1.1323326   0.5053259  -0.47694284\n",
      " -0.4776199   2.1435223   2.1463327   0.50270325  0.504127    0.5065464\n",
      "  0.1780656   0.17818199  0.1776922   1.8128796   2.4620082   2.4700983\n",
      "  0.17978846  0.17860691  0.17778818 -0.15088698 -0.1496682  -0.47803596\n",
      " -0.47847265 -0.47864795 -0.47896713 -0.8098023  -0.81029856 -0.15134294\n",
      " -0.15094681 -0.14971796 -0.8092103  -0.8084682  -0.15027104 -0.15053564\n",
      " -0.15050277 -0.80868095 -0.8100197  -0.8103086  -0.8092177  -0.47962183\n",
      " -0.47904605 -0.8082553  -0.8097852  -0.15127687 -0.14944962 -0.14948326\n",
      " -0.1506364  -0.48061374 -0.4791101   0.1767852   0.17765504 -0.4786923\n",
      " -0.47910172  0.5060803   1.1508847   1.1596876   0.5068464   0.50483507\n",
      "  0.83299696 -0.8106435  -0.8100286   0.83134824  0.83126074  0.17861882\n",
      " -0.80940753 -0.8096455  -0.8108241  -0.8105348   0.17963028  0.5059243\n",
      "  0.50630563  0.50792164 -0.4777733  -0.15136749 -0.14967163  0.17700525\n",
      " -0.4783274  -0.47771037 -0.47932282 -0.48010394 -0.4808719  -0.808897\n",
      " -0.809168   -1.1333477  -1.1334528  -1.134229   -1.134327   -1.1345748\n",
      " -1.1342863  -0.15032777 -1.1319101  -1.1314656  -0.15263441 -0.1519646\n",
      " -0.47942176  0.8264166   0.82742757  0.83030415  0.83237326 -0.14752781\n",
      " -0.14838275 -0.14898182 -1.1345246   0.50636154  0.5061915   0.5036226\n",
      " -0.8121515  -0.8125654  -0.8113726  -0.8093537  -0.15199345 -0.15125607\n",
      " -0.15037926 -0.14908431  0.5053815   0.5054893  -0.14983718 -0.14992283\n",
      " -0.15030035 -0.1513458  -0.15134016 -0.150666   -0.4801015  -0.47801828\n",
      " -0.47775984 -0.47776312 -0.4785722  -0.47790262 -0.4779136  -0.47758293\n",
      " -1.1323907   0.5051203   0.5066406   0.5065262  -0.47979107 -0.47979698\n",
      " -0.48037252 -0.48026556 -0.48103258 -0.47963095 -0.479609   -0.478497\n",
      " -0.47965923 -1.1315696  -1.1362191  -0.8108728  -0.8103312  -0.8108186\n",
      " -0.80942273 -0.809189   -0.8059746  -0.80476665 -0.8047041  -0.80883217\n",
      " -0.80956113 -0.4766285  -0.47525507 -0.4755894  -0.47641978 -0.4799574\n",
      " -0.8106485  -0.81033796  0.17867082 -0.15253301 -0.8085148  -0.8081144\n",
      " -0.80799115 -0.80891615 -0.81129485 -0.81144446 -0.8110713  -0.812154\n",
      " -0.8121172  -0.81130254 -0.8057138  -0.8051247  -0.8027191  -0.80320597\n",
      " -0.8005087  -0.8083289  -0.809025   -1.134118   -1.1354814  -1.1361738\n",
      " -0.14950608 -0.15028407 -0.47885576 -0.47957474 -0.47997993 -0.14901704\n",
      " -0.15054987 -0.8101618  -0.81088    -0.47923744 -0.47939992 -0.4785603\n",
      " -0.8094279  -0.47835752 -0.477986   -0.47850105 -0.47914234 -0.47943723\n",
      " -0.47993034 -0.4797278  -1.1318427  -0.47906345 -0.47872937  0.18150866\n",
      "  0.17749894  0.17672251 -0.15200672 -0.47803167 -0.47904694 -0.4797227\n",
      " -0.47920313 -0.4795327  -0.4786335   0.1788008   0.17818427 -0.80983967\n",
      " -0.80977863 -0.15014786 -0.1513999  -0.15151887 -0.1523938  -0.15203077\n",
      " -0.8090526  -0.8105414  -0.81039447 -0.8114636  -0.47894526 -0.47852838\n",
      " -0.47770956  0.50715536  0.5081223   0.18067715 -0.4777085  -0.477927\n",
      "  1.1570871   1.1557015   1.1563805   1.1584603   1.158384    0.50750744\n",
      "  2.7976406   2.7942274   2.7900152   5.1006713   5.082222    5.05845\n",
      "  4.1135697   5.748458    5.7498374   5.7593145   3.4622805   3.4510036\n",
      "  3.4430425   2.4631698   2.4640563   1.8117706   1.8104548   1.8007585\n",
      " -0.47875652 -0.47916692 -0.4794442  -0.47943726  0.18039283  0.1793228\n",
      "  0.17860968 -0.8097632  -0.8089085  -0.80858433 -0.15146524 -0.15200616\n",
      " -0.1519105   0.50496083  0.50584805  0.5065296  -0.15135443 -1.1327534\n",
      " -1.1327091  -0.80998826 -0.8071906  -0.80724156  0.1800135  -0.15139367\n",
      " -0.15218163 -1.1344303  -1.1332802  -1.1319824  -0.4787955  -0.47886017\n",
      " -0.48006082 -0.80872357 -0.8093562  -0.8096797  -0.47965702 -1.1352414\n",
      " -1.1352537  -1.1339508  -0.15046623 -0.15018259 -0.47764823 -0.47781086\n",
      " -0.4786081  -0.47793412 -0.4784402  -1.1347265  -1.1352949  -1.1340591\n",
      " -0.14993794 -0.15018362 -0.1501926  -0.15005562 -0.15019439 -0.15123521\n",
      " -0.15302016 -1.1366122  -1.1353064  -1.1328323  -0.4786868  -0.4791589\n",
      " -0.80874765 -0.8078618  -0.4804607  -0.48131764 -0.4805658  -0.47912443\n",
      "  0.51228446  0.5103426   0.50850266  0.5067575   0.18014169  0.17858773\n",
      "  0.17819417  0.17780389  0.18019238  0.50806105  0.5061478   0.50438243\n",
      "  0.17874047  0.17742985  2.7965126   2.7894886   1.1542169   1.1565591\n",
      "  1.1558845   1.1518507   1.1531413   1.155995   -0.47896922 -0.4807279\n",
      " -0.15491062 -0.15312304 -0.1512062  -0.4788431  -0.47765994  1.1580042\n",
      "  0.5003977   0.50206125  0.50482297  0.5084181   0.5053332   0.8295542\n",
      "  0.8344173   2.1390562   2.1370664   0.50549203  1.8092024   1.8108935\n",
      "  1.8108745   0.17666714  0.17676723  0.17698734  1.4850519   1.4818596\n",
      "  1.4792968   1.4784623  -0.47935823 -0.47875994  0.17759943  0.17820087\n",
      " -0.15165688 -0.8085611  -0.8098818  -0.81058013  0.5106142   0.5100768\n",
      "  0.50954545  0.50912094 -0.80779165 -0.8073588  -0.8069757  -0.4778167\n",
      " -0.47880062 -0.47943658  0.17992543  0.17861676  0.17733747 -0.8100989\n",
      " -0.809649   -0.80967104 -0.8097291  -0.47983232 -1.1366099  -1.1372107\n",
      "  0.17781615 -0.8095304  -0.808526   -0.80943483 -0.8098351  -0.15092278\n",
      " -0.15069665  0.8327153   0.8312174   0.83186394  0.8326662  -0.15274331\n",
      " -0.15200403 -0.15087523  0.50584924  1.4853652   1.4851224   0.5061917\n",
      "  1.4821073   1.1565518   1.155966    1.1525431   0.8311303   0.8338435\n",
      " -0.14634429 -0.14487053 -0.14558841 -0.14688163 -0.14736699  0.5063136\n",
      "  0.50723773  0.83095556  2.1377294   2.4673848  -0.15253465  0.83157235\n",
      "  0.8315532   0.8299316   0.50273395  0.5022687   0.17926931  0.18173884\n",
      "  0.18294443  0.18580638  0.50702673  0.50562656 -0.80838877 -0.8085468\n",
      " -0.8073039  -0.15155856 -0.15097441 -1.1334642  -1.1344149  -1.1344438\n",
      " -1.1325111  -0.47840402 -0.47885847 -0.4785     -0.47839254 -0.8102422\n",
      " -0.8094638  -0.15109158 -0.15217946 -0.15168992  0.50384486 -0.4807324\n",
      " -0.48049328 -0.47927403 -0.15295161 -0.15128446 -0.15051568 -0.4791629\n",
      " -0.47842175 -0.47713992  0.17715584  0.17774768  0.8338565   0.8363395\n",
      "  0.8365802  -0.151357   -0.15172513  0.5042822  -1.1321338  -1.1328714\n",
      "  1.4869865   1.4851639   1.483432    0.5089314   0.50829643  1.4825386\n",
      "  1.4893366   1.8147633   1.8144163   1.1532822   1.1532303   2.8033652\n",
      "  2.806949   -0.14992796 -0.15057863  0.50628424  0.5052385   0.5052258\n",
      "  0.507067    0.5059233  -0.15145342 -0.1505508  -1.1378895  -1.1378022\n",
      " -0.48017207 -0.48111874 -0.4807229  -0.48016948 -0.47852218 -0.4782966\n",
      " -0.47874472 -0.4793644  -0.47919193 -0.47918805 -1.1344526  -1.1341946\n",
      " -1.1333404  -0.47830403 -1.1335088  -1.134758   -1.1305068  -1.1302506\n",
      " -1.1321518  -1.1353755  -0.80767596 -0.8065673  -0.8071389  -0.8070485\n",
      " -0.8064666  -0.81056726 -0.15111107  0.50596434 -0.47870886 -0.8090943\n",
      " -0.81059164 -0.81121415  0.5083856   0.50250024  1.8101712   1.8090047\n",
      "  1.8071665  -0.80917776 -0.80967885 -0.8081306  -0.47861743 -0.47896034\n",
      " -0.4810368  -0.48113316 -0.47772655 -0.47818187 -0.47921282 -0.47929472\n",
      "  0.17888504  0.18017602  0.8298041   0.8285718   0.8305862   0.8326887\n",
      "  0.50911945 -0.8113964  -0.81055343  0.8328336   0.82953924  0.17805333\n",
      "  0.17791447  0.17558482 -0.48048803  0.5076243   0.5053034   0.50482094\n",
      "  0.18040897  0.17937788  0.1799061   0.83329403  0.8353519   0.8385056\n",
      " -0.4777488  -0.4784182  -0.81053305 -0.8102604   0.17686382  0.17832024\n",
      "  0.17925647  0.17878623  0.8321948   0.8290388  -0.15063153 -0.15075131\n",
      " -0.47801173 -0.47956342 -0.48006588 -0.47907978 -0.47877496 -0.80773103\n",
      " -0.8092977  -0.8099398  -0.1500625  -0.8090072  -0.15081258 -0.15093896\n",
      " -0.4788736  -0.47987047 -0.47808883 -0.47853202 -0.47787222 -0.47818863\n",
      " -0.15063547 -0.1512799  -0.15080649 -0.15157488 -0.15135138 -0.15190558\n",
      " -0.15163459 -0.48067862 -0.47935748]\n",
      "predicted_values[eval_mask[:, feature_idx].bool(), feature_idx].numpy() * scaler [-1.1329057  -1.1340879  -0.15247424  0.8308988  -0.14965835 -0.15176082\n",
      " -0.15173784 -1.1290559  -1.1306194  -1.1323326   0.5053259  -0.47694284\n",
      " -0.4776199   2.1435223   2.1463327   0.50270325  0.504127    0.5065464\n",
      "  0.1780656   0.17818199  0.1776922   1.8128796   2.4620082   2.4700983\n",
      "  0.17978846  0.17860691  0.17778818 -0.15088698 -0.1496682  -0.47803596\n",
      " -0.47847265 -0.47864795 -0.47896713 -0.8098023  -0.81029856 -0.15134294\n",
      " -0.15094681 -0.14971796 -0.8092103  -0.8084682  -0.15027104 -0.15053564\n",
      " -0.15050277 -0.80868095 -0.8100197  -0.8103086  -0.8092177  -0.47962183\n",
      " -0.47904605 -0.8082553  -0.8097852  -0.15127687 -0.14944962 -0.14948326\n",
      " -0.1506364  -0.48061374 -0.4791101   0.1767852   0.17765504 -0.4786923\n",
      " -0.47910172  0.5060803   1.1508847   1.1596876   0.5068464   0.50483507\n",
      "  0.83299696 -0.8106435  -0.8100286   0.83134824  0.83126074  0.17861882\n",
      " -0.80940753 -0.8096455  -0.8108241  -0.8105348   0.17963028  0.5059243\n",
      "  0.50630563  0.50792164 -0.4777733  -0.15136749 -0.14967163  0.17700525\n",
      " -0.4783274  -0.47771037 -0.47932282 -0.48010394 -0.4808719  -0.808897\n",
      " -0.809168   -1.1333477  -1.1334528  -1.134229   -1.134327   -1.1345748\n",
      " -1.1342863  -0.15032777 -1.1319101  -1.1314656  -0.15263441 -0.1519646\n",
      " -0.47942176  0.8264166   0.82742757  0.83030415  0.83237326 -0.14752781\n",
      " -0.14838275 -0.14898182 -1.1345246   0.50636154  0.5061915   0.5036226\n",
      " -0.8121515  -0.8125654  -0.8113726  -0.8093537  -0.15199345 -0.15125607\n",
      " -0.15037926 -0.14908431  0.5053815   0.5054893  -0.14983718 -0.14992283\n",
      " -0.15030035 -0.1513458  -0.15134016 -0.150666   -0.4801015  -0.47801828\n",
      " -0.47775984 -0.47776312 -0.4785722  -0.47790262 -0.4779136  -0.47758293\n",
      " -1.1323907   0.5051203   0.5066406   0.5065262  -0.47979107 -0.47979698\n",
      " -0.48037252 -0.48026556 -0.48103258 -0.47963095 -0.479609   -0.478497\n",
      " -0.47965923 -1.1315696  -1.1362191  -0.8108728  -0.8103312  -0.8108186\n",
      " -0.80942273 -0.809189   -0.8059746  -0.80476665 -0.8047041  -0.80883217\n",
      " -0.80956113 -0.4766285  -0.47525507 -0.4755894  -0.47641978 -0.4799574\n",
      " -0.8106485  -0.81033796  0.17867082 -0.15253301 -0.8085148  -0.8081144\n",
      " -0.80799115 -0.80891615 -0.81129485 -0.81144446 -0.8110713  -0.812154\n",
      " -0.8121172  -0.81130254 -0.8057138  -0.8051247  -0.8027191  -0.80320597\n",
      " -0.8005087  -0.8083289  -0.809025   -1.134118   -1.1354814  -1.1361738\n",
      " -0.14950608 -0.15028407 -0.47885576 -0.47957474 -0.47997993 -0.14901704\n",
      " -0.15054987 -0.8101618  -0.81088    -0.47923744 -0.47939992 -0.4785603\n",
      " -0.8094279  -0.47835752 -0.477986   -0.47850105 -0.47914234 -0.47943723\n",
      " -0.47993034 -0.4797278  -1.1318427  -0.47906345 -0.47872937  0.18150866\n",
      "  0.17749894  0.17672251 -0.15200672 -0.47803167 -0.47904694 -0.4797227\n",
      " -0.47920313 -0.4795327  -0.4786335   0.1788008   0.17818427 -0.80983967\n",
      " -0.80977863 -0.15014786 -0.1513999  -0.15151887 -0.1523938  -0.15203077\n",
      " -0.8090526  -0.8105414  -0.81039447 -0.8114636  -0.47894526 -0.47852838\n",
      " -0.47770956  0.50715536  0.5081223   0.18067715 -0.4777085  -0.477927\n",
      "  1.1570871   1.1557015   1.1563805   1.1584603   1.158384    0.50750744\n",
      "  2.7976406   2.7942274   2.7900152   5.1006713   5.082222    5.05845\n",
      "  4.1135697   5.748458    5.7498374   5.7593145   3.4622805   3.4510036\n",
      "  3.4430425   2.4631698   2.4640563   1.8117706   1.8104548   1.8007585\n",
      " -0.47875652 -0.47916692 -0.4794442  -0.47943726  0.18039283  0.1793228\n",
      "  0.17860968 -0.8097632  -0.8089085  -0.80858433 -0.15146524 -0.15200616\n",
      " -0.1519105   0.50496083  0.50584805  0.5065296  -0.15135443 -1.1327534\n",
      " -1.1327091  -0.80998826 -0.8071906  -0.80724156  0.1800135  -0.15139367\n",
      " -0.15218163 -1.1344303  -1.1332802  -1.1319824  -0.4787955  -0.47886017\n",
      " -0.48006082 -0.80872357 -0.8093562  -0.8096797  -0.47965702 -1.1352414\n",
      " -1.1352537  -1.1339508  -0.15046623 -0.15018259 -0.47764823 -0.47781086\n",
      " -0.4786081  -0.47793412 -0.4784402  -1.1347265  -1.1352949  -1.1340591\n",
      " -0.14993794 -0.15018362 -0.1501926  -0.15005562 -0.15019439 -0.15123521\n",
      " -0.15302016 -1.1366122  -1.1353064  -1.1328323  -0.4786868  -0.4791589\n",
      " -0.80874765 -0.8078618  -0.4804607  -0.48131764 -0.4805658  -0.47912443\n",
      "  0.51228446  0.5103426   0.50850266  0.5067575   0.18014169  0.17858773\n",
      "  0.17819417  0.17780389  0.18019238  0.50806105  0.5061478   0.50438243\n",
      "  0.17874047  0.17742985  2.7965126   2.7894886   1.1542169   1.1565591\n",
      "  1.1558845   1.1518507   1.1531413   1.155995   -0.47896922 -0.4807279\n",
      " -0.15491062 -0.15312304 -0.1512062  -0.4788431  -0.47765994  1.1580042\n",
      "  0.5003977   0.50206125  0.50482297  0.5084181   0.5053332   0.8295542\n",
      "  0.8344173   2.1390562   2.1370664   0.50549203  1.8092024   1.8108935\n",
      "  1.8108745   0.17666714  0.17676723  0.17698734  1.4850519   1.4818596\n",
      "  1.4792968   1.4784623  -0.47935823 -0.47875994  0.17759943  0.17820087\n",
      " -0.15165688 -0.8085611  -0.8098818  -0.81058013  0.5106142   0.5100768\n",
      "  0.50954545  0.50912094 -0.80779165 -0.8073588  -0.8069757  -0.4778167\n",
      " -0.47880062 -0.47943658  0.17992543  0.17861676  0.17733747 -0.8100989\n",
      " -0.809649   -0.80967104 -0.8097291  -0.47983232 -1.1366099  -1.1372107\n",
      "  0.17781615 -0.8095304  -0.808526   -0.80943483 -0.8098351  -0.15092278\n",
      " -0.15069665  0.8327153   0.8312174   0.83186394  0.8326662  -0.15274331\n",
      " -0.15200403 -0.15087523  0.50584924  1.4853652   1.4851224   0.5061917\n",
      "  1.4821073   1.1565518   1.155966    1.1525431   0.8311303   0.8338435\n",
      " -0.14634429 -0.14487053 -0.14558841 -0.14688163 -0.14736699  0.5063136\n",
      "  0.50723773  0.83095556  2.1377294   2.4673848  -0.15253465  0.83157235\n",
      "  0.8315532   0.8299316   0.50273395  0.5022687   0.17926931  0.18173884\n",
      "  0.18294443  0.18580638  0.50702673  0.50562656 -0.80838877 -0.8085468\n",
      " -0.8073039  -0.15155856 -0.15097441 -1.1334642  -1.1344149  -1.1344438\n",
      " -1.1325111  -0.47840402 -0.47885847 -0.4785     -0.47839254 -0.8102422\n",
      " -0.8094638  -0.15109158 -0.15217946 -0.15168992  0.50384486 -0.4807324\n",
      " -0.48049328 -0.47927403 -0.15295161 -0.15128446 -0.15051568 -0.4791629\n",
      " -0.47842175 -0.47713992  0.17715584  0.17774768  0.8338565   0.8363395\n",
      "  0.8365802  -0.151357   -0.15172513  0.5042822  -1.1321338  -1.1328714\n",
      "  1.4869865   1.4851639   1.483432    0.5089314   0.50829643  1.4825386\n",
      "  1.4893366   1.8147633   1.8144163   1.1532822   1.1532303   2.8033652\n",
      "  2.806949   -0.14992796 -0.15057863  0.50628424  0.5052385   0.5052258\n",
      "  0.507067    0.5059233  -0.15145342 -0.1505508  -1.1378895  -1.1378022\n",
      " -0.48017207 -0.48111874 -0.4807229  -0.48016948 -0.47852218 -0.4782966\n",
      " -0.47874472 -0.4793644  -0.47919193 -0.47918805 -1.1344526  -1.1341946\n",
      " -1.1333404  -0.47830403 -1.1335088  -1.134758   -1.1305068  -1.1302506\n",
      " -1.1321518  -1.1353755  -0.80767596 -0.8065673  -0.8071389  -0.8070485\n",
      " -0.8064666  -0.81056726 -0.15111107  0.50596434 -0.47870886 -0.8090943\n",
      " -0.81059164 -0.81121415  0.5083856   0.50250024  1.8101712   1.8090047\n",
      "  1.8071665  -0.80917776 -0.80967885 -0.8081306  -0.47861743 -0.47896034\n",
      " -0.4810368  -0.48113316 -0.47772655 -0.47818187 -0.47921282 -0.47929472\n",
      "  0.17888504  0.18017602  0.8298041   0.8285718   0.8305862   0.8326887\n",
      "  0.50911945 -0.8113964  -0.81055343  0.8328336   0.82953924  0.17805333\n",
      "  0.17791447  0.17558482 -0.48048803  0.5076243   0.5053034   0.50482094\n",
      "  0.18040897  0.17937788  0.1799061   0.83329403  0.8353519   0.8385056\n",
      " -0.4777488  -0.4784182  -0.81053305 -0.8102604   0.17686382  0.17832024\n",
      "  0.17925647  0.17878623  0.8321948   0.8290388  -0.15063153 -0.15075131\n",
      " -0.47801173 -0.47956342 -0.48006588 -0.47907978 -0.47877496 -0.80773103\n",
      " -0.8092977  -0.8099398  -0.1500625  -0.8090072  -0.15081258 -0.15093896\n",
      " -0.4788736  -0.47987047 -0.47808883 -0.47853202 -0.47787222 -0.47818863\n",
      " -0.15063547 -0.1512799  -0.15080649 -0.15157488 -0.15135138 -0.15190558\n",
      " -0.15163459 -0.48067862 -0.47935748]\n",
      "MSE for key X is: 0.05671465302718948\n",
      "MSE for key NIR is: 0.05368164207706557\n",
      "MSE for key IR is: 0.061753343820195254\n",
      "MSE for key Sub-mm is: 0.2635991477375798\n",
      "Results saved to model_results.npz\n",
      "Figure(1000x800)\n"
     ]
    }
   ],
   "source": [
    "!python exe_sgra.py --gp_noise --modelfolder sgra_fold0_final_20250404_110307"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(config='base.yaml', device='cuda:0', seed=1, testmissingratio=0.1, nfold=0, unconditional=False, gp_noise=False, modelfolder='sgra_fold0_final_20250404_110307', nsample=100, real_data_path='../Analysis/real_data.npz')\n",
      "{\n",
      "    \"train\": {\n",
      "        \"epochs\": 1,\n",
      "        \"batch_size\": 16,\n",
      "        \"lr\": 0.001\n",
      "    },\n",
      "    \"diffusion\": {\n",
      "        \"layers\": 8,\n",
      "        \"channels\": 128,\n",
      "        \"nheads\": 8,\n",
      "        \"diffusion_embedding_dim\": 256,\n",
      "        \"beta_start\": 0.0001,\n",
      "        \"beta_end\": 0.5,\n",
      "        \"num_steps\": 50,\n",
      "        \"schedule\": \"quad\"\n",
      "    },\n",
      "    \"model\": {\n",
      "        \"is_unconditional\": false,\n",
      "        \"timeemb\": 128,\n",
      "        \"featureemb\": 16,\n",
      "        \"target_strategy\": \"random\",\n",
      "        \"test_missing_ratio\": 0.1\n",
      "    }\n",
      "}\n",
      "/home/gsasseville/.local/share/virtualenvs/Diffusion-Nf9kVZlp/lib/python3.11/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "Loading pre-trained model from folder: ./save/sgra_fold0_final_20250404_110307/model.pth\n",
      "/home/gsasseville/Files/UDEM/Maitrise/SgrA/SgrA_Interpolation/Diffusion/exe_real.py:52: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"./save/\" + args.modelfolder + \"/model.pth\")\n",
      "Evaluating Model\n",
      "eval_mask[:, feature_idx].bool() tensor([False,  True,  True,  True,  True, False,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True,  True,\n",
      "        False,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True,  True,\n",
      "        False,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True,  True, False,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True,  True, False,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True,  True,\n",
      "        False,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True,  True, False,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True])\n",
      "predicted_values[eval_mask[:, feature_idx].bool(), feature_idx].numpy() [-0.598573   -0.59895486 -0.5983511  -0.5978808  -0.2699249  -0.26966122\n",
      " -0.2696311  -0.26933503 -0.26969668 -0.27062678 -0.27094853 -0.26981214\n",
      " -0.270197   -0.26913983 -0.26946533 -0.26926494  0.5669523   0.5700326\n",
      "  0.5717826  -0.59997517 -0.6002024  -0.60031575 -0.598764   -0.5975026\n",
      " -0.5971012  -0.5982931  -0.5991958  -0.26626113 -0.26644033 -0.26591435\n",
      " -0.26683393 -0.2681666  -0.59668684 -0.5965812  -0.59747696  0.07378361\n",
      "  0.07228219  0.07120324  0.07129859 -0.26373735 -0.26630282 -0.2662536\n",
      " -0.26544118 -0.26637626 -0.59887403 -0.59822565 -0.59828335  0.4099503\n",
      "  0.41150448  0.41264865  0.41149393 -0.27162337 -0.271792   -0.27147356\n",
      " -0.26990128 -0.26931694 -0.2704449  -0.26851183 -0.27075598 -0.27027667\n",
      " -0.26577482 -0.26308987  1.0658963  -0.2663831  -0.26785174 -0.26819855\n",
      " -0.26956144  1.082271    1.083704    1.0842406   1.0840468   0.07450525\n",
      "  0.07482214  0.07367467  0.07365625  0.4097482   0.4101263   0.4112367\n",
      "  0.41132984 -0.5940649  -0.59510034 -0.5978921  -0.59669834 -0.28192535\n",
      " -0.27942055 -0.28333944 -0.28000185 -0.27829847  0.06615468  0.06900179\n",
      "  0.06749678  0.07401854  0.07382929  0.07522616  0.07504695 -0.26972523\n",
      " -0.27030247 -0.2692186  -0.2694586  -0.59867644 -0.5987215  -0.59909046\n",
      " -0.5984906   0.41034216  0.40800756  0.40905076  0.41065472  0.07432973\n",
      "  0.07458664  0.07487645  0.07400489 -0.5883848  -0.5900371  -0.5905143\n",
      " -0.5873485  -0.5883077   0.24274027  0.20369653  0.3196952  -0.59677035\n",
      " -0.5984008  -0.5976777  -0.5984333  -0.5985062  -0.5977674  -0.5975039\n",
      " -0.5967294   0.7349846   0.7378693   0.7383532   0.7372852  -0.59838915\n",
      " -0.59874684 -0.5987619  -0.59944606 -0.27005962 -0.2705793  -0.27059498\n",
      " -0.27002797 -0.26985568 -0.2695296  -0.26888132 -0.2681874  -0.26802474\n",
      " -0.2685748  -0.26954657 -0.26938617 -0.26854107 -0.269301   -0.26878363\n",
      " -0.26801777 -0.2671644  -0.26741275 -0.2668444  -0.26719612 -0.2690699\n",
      " -0.2711804  -0.27047765 -0.27203855  0.7396042   0.7407975   0.7406082\n",
      "  0.7381222   0.4102203   0.40968698  0.4097405   0.41080683 -0.26633754\n",
      " -0.2659004  -0.26595262 -0.26514155  0.40958112  0.40492007  0.40523964\n",
      "  0.40820757  0.07507265  0.07378531  0.07234327  0.07297755  0.73352265\n",
      "  0.7359269   0.738639    0.7394459   5.7616644   5.750214    5.7348933\n",
      "  5.7359      8.689344    8.720584    8.753292    8.862357    3.7339966\n",
      "  3.744733    3.749121    3.7532828   2.426452    2.4267185   2.4336016\n",
      "  2.4353967  -0.59538203 -0.59838015 -0.5993134  -0.6000255   0.40849635\n",
      "  0.40693897  0.40688145  0.40747103 -0.26993564 -0.26979744 -0.27140188\n",
      " -0.27038687  0.07396954  0.0753976   0.07676789  0.07687232 -0.26809728\n",
      " -0.26759303 -0.26835757 -0.26822284 -0.2675779  -0.2676686  -0.26942933\n",
      " -0.26962158 -0.27042094 -0.26982355 -0.27076572 -0.269081    1.0870605\n",
      "  1.0890555   1.0902411   1.0879734  -0.5993088  -0.60045975 -0.59903973\n",
      " -0.59851754 -0.27009696 -0.2693522  -0.26924205 -0.2692251   0.4076061\n",
      "  0.40824944  0.40804565  0.40799072  0.40838084  0.4094209   0.41017342\n",
      "  0.4094542  -0.2708864  -0.26971212 -0.26904762 -0.26853916  0.07519135\n",
      "  0.07538243  0.07548803  0.07608133 -0.26939866 -0.26874226 -0.26830095\n",
      " -0.26941708  0.07942418  0.07887965  0.07845526  0.07712802  0.07500412\n",
      "  0.07403985  0.07400645  0.07438815  0.07237233  0.07235105  0.07291114\n",
      "  0.07318434 -0.26694098 -0.26875436 -0.26792637 -0.26988864  0.07508448\n",
      "  0.07596516  0.07601531  0.07576996 -0.26910812 -0.26964    -0.26955327\n",
      " -0.26990548 -0.2695318  -0.2684531  -0.26827487 -0.2684013   1.4229633\n",
      "  1.4166797   1.4162087   1.4120414  -0.59906423 -0.59828407 -0.5984354\n",
      " -0.5992994  -0.26800582 -0.26868024 -0.26990128 -0.26971424 -0.5983539\n",
      " -0.5984555  -0.598205   -0.59762126 -0.26797327 -0.26693666 -0.26715928\n",
      " -0.2671987  -0.5973208  -0.5968454  -0.5967643  -0.5969736   0.40709582\n",
      "  0.4050677   0.40454364  0.40479466  0.07601446  0.07565089  0.0747095\n",
      "  0.07360914  0.07170182  0.07281762  0.07386759  0.07493994  0.40475604\n",
      "  0.4068549   0.40682992  0.408509    0.07811506  0.07856817  0.07820564\n",
      "  0.07724421  0.0754854   0.07647493  0.07648814  0.07620692  0.07453069\n",
      "  0.07536419  0.07507053  0.07407448 -0.26803568 -0.2677407  -0.26742402\n",
      " -0.26739123 -0.2698748  -0.26981187 -0.26971826 -0.2690035  -0.27002043\n",
      " -0.27030107 -0.2701273  -0.2704427  -0.27109426 -0.26978892 -0.26978642\n",
      " -0.268601   -0.26834768 -0.2679849  -0.26877847 -0.26882178 -0.5984358\n",
      " -0.5995085  -0.59952146 -0.59901476 -0.26968813 -0.27008867 -0.26972207\n",
      " -0.26968822  0.40798154  0.40946925  0.41111413  0.41231123 -0.26754373\n",
      " -0.26809767 -0.26839152 -0.26834452 -0.26731542 -0.26735905 -0.26686576\n",
      " -0.26821288 -0.5975941  -0.59774196 -0.5977147  -0.59599704  0.7410565\n",
      "  0.7399098   0.7397549   0.73860466  0.4149486   0.41205704  0.411389\n",
      "  0.4117141  -0.59549093 -0.5955894  -0.59673774 -0.59819436  0.40983236\n",
      "  0.40777403  0.40905005  0.40707603  0.4078373   0.40972733  0.41094628\n",
      "  0.41055575  0.07415777  0.07249443  0.07397535  0.07430404  0.07342271\n",
      "  0.07251909  0.07460017  0.0755633  -0.26814464 -0.27015635 -0.27037203\n",
      " -0.2694544   0.41502684  0.41431057  0.4128671   0.41227624 -0.26821014\n",
      " -0.26714772 -0.2685197  -0.26905268 -0.595871   -0.5963981  -0.59667045\n",
      " -0.5977344   0.74090743  0.74221474  0.74325734  0.74369806  0.07210162\n",
      "  0.07188439  0.07294293  0.07435001 -0.59800506 -0.59764355 -0.596987\n",
      " -0.59806556 -0.5961935  -0.59592986 -0.59671235 -0.59747785 -0.26867878\n",
      " -0.2689013  -0.26767233 -0.26875332 -0.26701614 -0.2689936  -0.26808435\n",
      " -0.26763827 -0.5968427  -0.5978151  -0.5970949  -0.59782124  0.07340419\n",
      "  0.07467633  0.0739248   0.07381596  0.07519478  0.07640867  0.07660841\n",
      "  0.07671668 -0.26817957 -0.26900414 -0.26767808 -0.26818782  0.07504274\n",
      "  0.07571856  0.07555166  0.07409693  0.7373312   0.7370133   0.7385089\n",
      "  0.7417033  -0.59597397 -0.59677756 -0.59771097 -0.5977052   0.07544215\n",
      "  0.07439946  0.07436682  0.07547715 -0.59864134 -0.5981259  -0.5970728\n",
      " -0.59762913  0.0740125   0.07498337  0.07561965  0.07595742  1.0799327\n",
      "  1.080204    1.0832734   1.0847937   1.0862273   1.0861291   1.0834233\n",
      "  1.081294   -0.5995861  -0.5988031  -0.5991808  -0.5982418   0.07324373\n",
      "  0.07230039  0.07182577  0.07177977  0.0733941   0.07507845  0.07712363\n",
      "  0.07660197  0.07231253  0.0705416   0.07074344  0.07184749  0.40881217\n",
      "  0.40921387  0.40692526  0.40712556 -0.6000742  -0.59850734 -0.59723777\n",
      " -0.5968738   0.4111197   0.41082537  0.41427547  0.41362768 -0.59710616\n",
      " -0.59787714 -0.59888613 -0.5981714  -0.27028432 -0.27065665 -0.26998258\n",
      " -0.26930082  0.07438719  0.07433669  0.07490796  0.07535987 -0.5994825\n",
      " -0.59881157 -0.59868187 -0.5978582  -0.26934117 -0.2687699  -0.26953885\n",
      " -0.2695991   0.7393931   0.7393793   0.7393306   0.73889744 -0.2679607\n",
      " -0.26740858 -0.26775834 -0.26648995 -0.26705417 -0.2677824  -0.26771185\n",
      " -0.26733115 -0.26760128 -0.2660701  -0.26659998 -0.2670405  -0.5960702\n",
      " -0.5970221  -0.59728444 -0.59799755 -0.26917908 -0.26867568 -0.26825827\n",
      " -0.26758352  0.07565741  0.07513056  0.07319846  0.07190526 -0.596487\n",
      " -0.5961585  -0.59456325 -0.5957926  -0.5954801  -0.5955773  -0.59633154\n",
      " -0.59693396  0.07402666  0.0747555   0.07385001  0.07428744 -0.26400158\n",
      " -0.2675657  -0.2683927  -0.26809925 -0.59828645 -0.59877586 -0.599451\n",
      " -0.599611   -0.59886885 -0.59881496 -0.5997578  -0.6011592  -0.59939605\n",
      " -0.59810674 -0.59859157 -0.59774673 -0.26961848 -0.26927212 -0.27009407\n",
      " -0.26943034  0.07313699  0.0745891   0.07453607  0.07465193 -0.5988412\n",
      " -0.59883296 -0.59857476 -0.5987705  -0.2663831  -0.26699284 -0.265951\n",
      " -0.2677093  -0.59798515 -0.5988511  -0.59694576 -0.5965534   0.4068664\n",
      "  0.4091867   0.40946525  0.41142684 -0.26733658 -0.26861238 -0.26891756\n",
      " -0.27095872 -0.26925644 -0.26829195 -0.2688354  -0.2683755  -0.59621507\n",
      " -0.596091   -0.5968475  -0.59871686  0.41397882  0.41528568  0.41497275\n",
      "  0.41236672 -0.26856348 -0.2693445  -0.26889032 -0.2682027  -0.2658043\n",
      " -0.265459   -0.26567888 -0.26604128 -0.2677153  -0.2669322  -0.2669562\n",
      " -0.26844618 -0.5949175  -0.59594035 -0.5961706  -0.59692    -0.5986121\n",
      " -0.5976681  -0.5979733  -0.5974812  -0.2699036  -0.2703986  -0.27025646\n",
      " -0.27038574 -0.598577   -0.59902316 -0.5981651  -0.5988926  -0.27159816\n",
      " -0.27036893 -0.26985747 -0.2705333  -0.59987813 -0.5985494  -0.5979766\n",
      " -0.5971137  -0.2667991  -0.2674977  -0.2680703  -0.2680473  -0.5982609\n",
      " -0.59692204 -0.59732866 -0.59785336  0.07431128  0.07416563  0.07501774\n",
      "  0.07459629 -0.5954351  -0.5964899  -0.59834313 -0.59837866 -0.26870093\n",
      " -0.26814863 -0.26784328 -0.2678202   0.4094353   0.41126594  0.40910754\n",
      "  0.40921924 -0.26947832 -0.2691268  -0.26937333 -0.269097   -0.26874387\n",
      " -0.26868448 -0.2681562  -0.2674621  -0.26809448 -0.2690548  -0.2691103\n",
      " -0.26799238  0.07425141  0.07446432  0.07591026  0.07632626 -0.5970618\n",
      " -0.59703875 -0.59677947 -0.5970615  -0.26998055 -0.2699856  -0.26949093\n",
      " -0.2694591  -0.5982844  -0.5976636  -0.59630513 -0.5962192  -0.59866995\n",
      " -0.5981708  -0.59905386 -0.59887594 -0.2696241  -0.26862624 -0.2686114\n",
      " -0.2681001   0.07452439  0.0732266   0.07346748  0.07272219 -0.5977345\n",
      " -0.59745175 -0.597374   -0.59805924  0.07406348  0.07354201  0.07361081\n",
      "  0.07329486 -0.26689944 -0.26826096 -0.2686013  -0.26850963  0.41454574\n",
      "  0.41385478  0.4137027   0.41351727 -0.26473215 -0.26506886 -0.26348957\n",
      " -0.26402754 -0.26230204 -0.26830354 -0.2689427  -0.26877552 -0.26765364\n",
      " -0.26812726 -0.26928928 -0.2692939  -0.2706706  -0.2706192 ]\n",
      "predicted_values[eval_mask[:, feature_idx].bool(), feature_idx].numpy() * scaler [-0.598573   -0.59895486 -0.5983511  -0.5978808  -0.2699249  -0.26966122\n",
      " -0.2696311  -0.26933503 -0.26969668 -0.27062678 -0.27094853 -0.26981214\n",
      " -0.270197   -0.26913983 -0.26946533 -0.26926494  0.5669523   0.5700326\n",
      "  0.5717826  -0.59997517 -0.6002024  -0.60031575 -0.598764   -0.5975026\n",
      " -0.5971012  -0.5982931  -0.5991958  -0.26626113 -0.26644033 -0.26591435\n",
      " -0.26683393 -0.2681666  -0.59668684 -0.5965812  -0.59747696  0.07378361\n",
      "  0.07228219  0.07120324  0.07129859 -0.26373735 -0.26630282 -0.2662536\n",
      " -0.26544118 -0.26637626 -0.59887403 -0.59822565 -0.59828335  0.4099503\n",
      "  0.41150448  0.41264865  0.41149393 -0.27162337 -0.271792   -0.27147356\n",
      " -0.26990128 -0.26931694 -0.2704449  -0.26851183 -0.27075598 -0.27027667\n",
      " -0.26577482 -0.26308987  1.0658963  -0.2663831  -0.26785174 -0.26819855\n",
      " -0.26956144  1.082271    1.083704    1.0842406   1.0840468   0.07450525\n",
      "  0.07482214  0.07367467  0.07365625  0.4097482   0.4101263   0.4112367\n",
      "  0.41132984 -0.5940649  -0.59510034 -0.5978921  -0.59669834 -0.28192535\n",
      " -0.27942055 -0.28333944 -0.28000185 -0.27829847  0.06615468  0.06900179\n",
      "  0.06749678  0.07401854  0.07382929  0.07522616  0.07504695 -0.26972523\n",
      " -0.27030247 -0.2692186  -0.2694586  -0.59867644 -0.5987215  -0.59909046\n",
      " -0.5984906   0.41034216  0.40800756  0.40905076  0.41065472  0.07432973\n",
      "  0.07458664  0.07487645  0.07400489 -0.5883848  -0.5900371  -0.5905143\n",
      " -0.5873485  -0.5883077   0.24274027  0.20369653  0.3196952  -0.59677035\n",
      " -0.5984008  -0.5976777  -0.5984333  -0.5985062  -0.5977674  -0.5975039\n",
      " -0.5967294   0.7349846   0.7378693   0.7383532   0.7372852  -0.59838915\n",
      " -0.59874684 -0.5987619  -0.59944606 -0.27005962 -0.2705793  -0.27059498\n",
      " -0.27002797 -0.26985568 -0.2695296  -0.26888132 -0.2681874  -0.26802474\n",
      " -0.2685748  -0.26954657 -0.26938617 -0.26854107 -0.269301   -0.26878363\n",
      " -0.26801777 -0.2671644  -0.26741275 -0.2668444  -0.26719612 -0.2690699\n",
      " -0.2711804  -0.27047765 -0.27203855  0.7396042   0.7407975   0.7406082\n",
      "  0.7381222   0.4102203   0.40968698  0.4097405   0.41080683 -0.26633754\n",
      " -0.2659004  -0.26595262 -0.26514155  0.40958112  0.40492007  0.40523964\n",
      "  0.40820757  0.07507265  0.07378531  0.07234327  0.07297755  0.73352265\n",
      "  0.7359269   0.738639    0.7394459   5.7616644   5.750214    5.7348933\n",
      "  5.7359      8.689344    8.720584    8.753292    8.862357    3.7339966\n",
      "  3.744733    3.749121    3.7532828   2.426452    2.4267185   2.4336016\n",
      "  2.4353967  -0.59538203 -0.59838015 -0.5993134  -0.6000255   0.40849635\n",
      "  0.40693897  0.40688145  0.40747103 -0.26993564 -0.26979744 -0.27140188\n",
      " -0.27038687  0.07396954  0.0753976   0.07676789  0.07687232 -0.26809728\n",
      " -0.26759303 -0.26835757 -0.26822284 -0.2675779  -0.2676686  -0.26942933\n",
      " -0.26962158 -0.27042094 -0.26982355 -0.27076572 -0.269081    1.0870605\n",
      "  1.0890555   1.0902411   1.0879734  -0.5993088  -0.60045975 -0.59903973\n",
      " -0.59851754 -0.27009696 -0.2693522  -0.26924205 -0.2692251   0.4076061\n",
      "  0.40824944  0.40804565  0.40799072  0.40838084  0.4094209   0.41017342\n",
      "  0.4094542  -0.2708864  -0.26971212 -0.26904762 -0.26853916  0.07519135\n",
      "  0.07538243  0.07548803  0.07608133 -0.26939866 -0.26874226 -0.26830095\n",
      " -0.26941708  0.07942418  0.07887965  0.07845526  0.07712802  0.07500412\n",
      "  0.07403985  0.07400645  0.07438815  0.07237233  0.07235105  0.07291114\n",
      "  0.07318434 -0.26694098 -0.26875436 -0.26792637 -0.26988864  0.07508448\n",
      "  0.07596516  0.07601531  0.07576996 -0.26910812 -0.26964    -0.26955327\n",
      " -0.26990548 -0.2695318  -0.2684531  -0.26827487 -0.2684013   1.4229633\n",
      "  1.4166797   1.4162087   1.4120414  -0.59906423 -0.59828407 -0.5984354\n",
      " -0.5992994  -0.26800582 -0.26868024 -0.26990128 -0.26971424 -0.5983539\n",
      " -0.5984555  -0.598205   -0.59762126 -0.26797327 -0.26693666 -0.26715928\n",
      " -0.2671987  -0.5973208  -0.5968454  -0.5967643  -0.5969736   0.40709582\n",
      "  0.4050677   0.40454364  0.40479466  0.07601446  0.07565089  0.0747095\n",
      "  0.07360914  0.07170182  0.07281762  0.07386759  0.07493994  0.40475604\n",
      "  0.4068549   0.40682992  0.408509    0.07811506  0.07856817  0.07820564\n",
      "  0.07724421  0.0754854   0.07647493  0.07648814  0.07620692  0.07453069\n",
      "  0.07536419  0.07507053  0.07407448 -0.26803568 -0.2677407  -0.26742402\n",
      " -0.26739123 -0.2698748  -0.26981187 -0.26971826 -0.2690035  -0.27002043\n",
      " -0.27030107 -0.2701273  -0.2704427  -0.27109426 -0.26978892 -0.26978642\n",
      " -0.268601   -0.26834768 -0.2679849  -0.26877847 -0.26882178 -0.5984358\n",
      " -0.5995085  -0.59952146 -0.59901476 -0.26968813 -0.27008867 -0.26972207\n",
      " -0.26968822  0.40798154  0.40946925  0.41111413  0.41231123 -0.26754373\n",
      " -0.26809767 -0.26839152 -0.26834452 -0.26731542 -0.26735905 -0.26686576\n",
      " -0.26821288 -0.5975941  -0.59774196 -0.5977147  -0.59599704  0.7410565\n",
      "  0.7399098   0.7397549   0.73860466  0.4149486   0.41205704  0.411389\n",
      "  0.4117141  -0.59549093 -0.5955894  -0.59673774 -0.59819436  0.40983236\n",
      "  0.40777403  0.40905005  0.40707603  0.4078373   0.40972733  0.41094628\n",
      "  0.41055575  0.07415777  0.07249443  0.07397535  0.07430404  0.07342271\n",
      "  0.07251909  0.07460017  0.0755633  -0.26814464 -0.27015635 -0.27037203\n",
      " -0.2694544   0.41502684  0.41431057  0.4128671   0.41227624 -0.26821014\n",
      " -0.26714772 -0.2685197  -0.26905268 -0.595871   -0.5963981  -0.59667045\n",
      " -0.5977344   0.74090743  0.74221474  0.74325734  0.74369806  0.07210162\n",
      "  0.07188439  0.07294293  0.07435001 -0.59800506 -0.59764355 -0.596987\n",
      " -0.59806556 -0.5961935  -0.59592986 -0.59671235 -0.59747785 -0.26867878\n",
      " -0.2689013  -0.26767233 -0.26875332 -0.26701614 -0.2689936  -0.26808435\n",
      " -0.26763827 -0.5968427  -0.5978151  -0.5970949  -0.59782124  0.07340419\n",
      "  0.07467633  0.0739248   0.07381596  0.07519478  0.07640867  0.07660841\n",
      "  0.07671668 -0.26817957 -0.26900414 -0.26767808 -0.26818782  0.07504274\n",
      "  0.07571856  0.07555166  0.07409693  0.7373312   0.7370133   0.7385089\n",
      "  0.7417033  -0.59597397 -0.59677756 -0.59771097 -0.5977052   0.07544215\n",
      "  0.07439946  0.07436682  0.07547715 -0.59864134 -0.5981259  -0.5970728\n",
      " -0.59762913  0.0740125   0.07498337  0.07561965  0.07595742  1.0799327\n",
      "  1.080204    1.0832734   1.0847937   1.0862273   1.0861291   1.0834233\n",
      "  1.081294   -0.5995861  -0.5988031  -0.5991808  -0.5982418   0.07324373\n",
      "  0.07230039  0.07182577  0.07177977  0.0733941   0.07507845  0.07712363\n",
      "  0.07660197  0.07231253  0.0705416   0.07074344  0.07184749  0.40881217\n",
      "  0.40921387  0.40692526  0.40712556 -0.6000742  -0.59850734 -0.59723777\n",
      " -0.5968738   0.4111197   0.41082537  0.41427547  0.41362768 -0.59710616\n",
      " -0.59787714 -0.59888613 -0.5981714  -0.27028432 -0.27065665 -0.26998258\n",
      " -0.26930082  0.07438719  0.07433669  0.07490796  0.07535987 -0.5994825\n",
      " -0.59881157 -0.59868187 -0.5978582  -0.26934117 -0.2687699  -0.26953885\n",
      " -0.2695991   0.7393931   0.7393793   0.7393306   0.73889744 -0.2679607\n",
      " -0.26740858 -0.26775834 -0.26648995 -0.26705417 -0.2677824  -0.26771185\n",
      " -0.26733115 -0.26760128 -0.2660701  -0.26659998 -0.2670405  -0.5960702\n",
      " -0.5970221  -0.59728444 -0.59799755 -0.26917908 -0.26867568 -0.26825827\n",
      " -0.26758352  0.07565741  0.07513056  0.07319846  0.07190526 -0.596487\n",
      " -0.5961585  -0.59456325 -0.5957926  -0.5954801  -0.5955773  -0.59633154\n",
      " -0.59693396  0.07402666  0.0747555   0.07385001  0.07428744 -0.26400158\n",
      " -0.2675657  -0.2683927  -0.26809925 -0.59828645 -0.59877586 -0.599451\n",
      " -0.599611   -0.59886885 -0.59881496 -0.5997578  -0.6011592  -0.59939605\n",
      " -0.59810674 -0.59859157 -0.59774673 -0.26961848 -0.26927212 -0.27009407\n",
      " -0.26943034  0.07313699  0.0745891   0.07453607  0.07465193 -0.5988412\n",
      " -0.59883296 -0.59857476 -0.5987705  -0.2663831  -0.26699284 -0.265951\n",
      " -0.2677093  -0.59798515 -0.5988511  -0.59694576 -0.5965534   0.4068664\n",
      "  0.4091867   0.40946525  0.41142684 -0.26733658 -0.26861238 -0.26891756\n",
      " -0.27095872 -0.26925644 -0.26829195 -0.2688354  -0.2683755  -0.59621507\n",
      " -0.596091   -0.5968475  -0.59871686  0.41397882  0.41528568  0.41497275\n",
      "  0.41236672 -0.26856348 -0.2693445  -0.26889032 -0.2682027  -0.2658043\n",
      " -0.265459   -0.26567888 -0.26604128 -0.2677153  -0.2669322  -0.2669562\n",
      " -0.26844618 -0.5949175  -0.59594035 -0.5961706  -0.59692    -0.5986121\n",
      " -0.5976681  -0.5979733  -0.5974812  -0.2699036  -0.2703986  -0.27025646\n",
      " -0.27038574 -0.598577   -0.59902316 -0.5981651  -0.5988926  -0.27159816\n",
      " -0.27036893 -0.26985747 -0.2705333  -0.59987813 -0.5985494  -0.5979766\n",
      " -0.5971137  -0.2667991  -0.2674977  -0.2680703  -0.2680473  -0.5982609\n",
      " -0.59692204 -0.59732866 -0.59785336  0.07431128  0.07416563  0.07501774\n",
      "  0.07459629 -0.5954351  -0.5964899  -0.59834313 -0.59837866 -0.26870093\n",
      " -0.26814863 -0.26784328 -0.2678202   0.4094353   0.41126594  0.40910754\n",
      "  0.40921924 -0.26947832 -0.2691268  -0.26937333 -0.269097   -0.26874387\n",
      " -0.26868448 -0.2681562  -0.2674621  -0.26809448 -0.2690548  -0.2691103\n",
      " -0.26799238  0.07425141  0.07446432  0.07591026  0.07632626 -0.5970618\n",
      " -0.59703875 -0.59677947 -0.5970615  -0.26998055 -0.2699856  -0.26949093\n",
      " -0.2694591  -0.5982844  -0.5976636  -0.59630513 -0.5962192  -0.59866995\n",
      " -0.5981708  -0.59905386 -0.59887594 -0.2696241  -0.26862624 -0.2686114\n",
      " -0.2681001   0.07452439  0.0732266   0.07346748  0.07272219 -0.5977345\n",
      " -0.59745175 -0.597374   -0.59805924  0.07406348  0.07354201  0.07361081\n",
      "  0.07329486 -0.26689944 -0.26826096 -0.2686013  -0.26850963  0.41454574\n",
      "  0.41385478  0.4137027   0.41351727 -0.26473215 -0.26506886 -0.26348957\n",
      " -0.26402754 -0.26230204 -0.26830354 -0.2689427  -0.26877552 -0.26765364\n",
      " -0.26812726 -0.26928928 -0.2692939  -0.2706706  -0.2706192 ]\n",
      "MSE for key X is: 1.2059682276085462\n",
      "MSE for key NIR is: 1.1428969594594593\n",
      "MSE for key IR is: 0.1194153700802884\n",
      "MSE for key Sub-mm is: 413.1210175552666\n",
      "Results saved to real_results.npz\n",
      "Figure(1000x800)\n"
     ]
    }
   ],
   "source": [
    "!python exe_real.py --modelfolder sgra_fold0_final_20250404_110307 --real_data_path ../Analysis/real_data.npz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Diffusion-Nf9kVZlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
