{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Parallel-oHipqXVA (Python 3.9.2)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/home/gsasseville/.local/share/virtualenvs/Parallel-oHipqXVA/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import lc_model as model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "class Simulation:\n",
    "    def __init__(self, epoch, sampling_rate, VarDict, num_simulations, batch_size=100, checkpoint_file=\"checkpoint.pkl\", worker_id=None):\n",
    "        self.epoch = epoch\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.VarDict = VarDict\n",
    "        self.num_simulations = num_simulations\n",
    "        self.batch_size = batch_size\n",
    "        self.worker_id = worker_id\n",
    "\n",
    "        # Update checkpoint file with worker ID to ensure uniqueness\n",
    "        if self.worker_id is not None:\n",
    "            self.checkpoint_file = f\"checkpoint_worker_{self.worker_id}.pkl\"\n",
    "        else:\n",
    "            self.checkpoint_file = checkpoint_file\n",
    "        \n",
    "        self.start_simulation = 0\n",
    "        self.batch_id = 0\n",
    "        \n",
    "        # Load checkpoint if it exists\n",
    "        self.load_checkpoint()\n",
    "\n",
    "    def run_simulation(self, sim_id):\n",
    "        # Simulation logic (adjust as needed for individual sim_id)\n",
    "        time = self.epoch * self.sampling_rate\n",
    "\n",
    "        # Each worker has its own ParticleSystem based on sim_id for unique runs\n",
    "        run = model.ParticleSystem(f\"run_{sim_id}.particle\", delPoints=1000)\n",
    "\n",
    "        run.ParamSet(self.VarDict, PickParticle=True)\n",
    "        run.params_model.update({\"noise_22GHz\": 0.02})\n",
    "\n",
    "        generator = model.DataGenerator(run.params_model, time)\n",
    "        generator.ModelSetup()\n",
    "\n",
    "        generator.LightCurveData(self.sampling_rate)\n",
    "        generator.CalculateLightCurves(0, time)\n",
    "\n",
    "        data = generator.masks(0.2)\n",
    "        \n",
    "        return {\"simulation_id\": sim_id, \"data\": data}\n",
    "\n",
    "    def run_and_save_simulations(self):\n",
    "        # Run the simulations in batches and save the results to files\n",
    "        while self.start_simulation < self.num_simulations:\n",
    "            end_simulation = min(self.start_simulation + self.batch_size, self.num_simulations)\n",
    "            batch_data = [self.run_simulation(sim_id) for sim_id in range(self.start_simulation, end_simulation)]\n",
    "            \n",
    "            # Save the batch data\n",
    "            self.save_data_batch(batch_data, self.batch_id)\n",
    "\n",
    "            # Update checkpoint and batch ID\n",
    "            self.batch_id += 1\n",
    "            self.start_simulation = end_simulation\n",
    "            self.save_checkpoint()\n",
    "\n",
    "    def save_data_batch(self, batch_data, batch_id):\n",
    "        # Use worker ID in the batch file name to prevent overwriting\n",
    "        if self.worker_id is not None:\n",
    "            file_name = f\"data_batch_worker_{self.worker_id}_{batch_id}.pkl\"\n",
    "        else:\n",
    "            file_name = f\"data_batch_{batch_id}.pkl\"\n",
    "        \n",
    "        with open(file_name, \"wb\") as f:\n",
    "            pickle.dump(batch_data, f)\n",
    "        print(f\"Saved {file_name}\")\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        checkpoint_data = {\n",
    "            \"start_simulation\": self.start_simulation,\n",
    "            \"batch_id\": self.batch_id\n",
    "        }\n",
    "        with open(self.checkpoint_file, \"wb\") as f:\n",
    "            pickle.dump(checkpoint_data, f)\n",
    "        print(f\"Checkpoint saved for worker {self.worker_id}.\")\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        if os.path.exists(self.checkpoint_file):\n",
    "            with open(self.checkpoint_file, \"rb\") as f:\n",
    "                checkpoint_data = pickle.load(f)\n",
    "                self.start_simulation = checkpoint_data[\"start_simulation\"]\n",
    "                self.batch_id = checkpoint_data[\"batch_id\"]\n",
    "            print(f\"Checkpoint loaded for worker {self.worker_id}.\")\n",
    "        else:\n",
    "            print(f\"No checkpoint found for worker {self.worker_id}. Starting from the beginning.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load_data_batch(batch_id, worker_id=None):\n",
    "        if worker_id is not None:\n",
    "            file_name = f\"data_batch_worker_{worker_id}_{batch_id}.pkl\"\n",
    "        else:\n",
    "            file_name = f\"data_batch_{batch_id}.pkl\"\n",
    "        \n",
    "        with open(file_name, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_all_data(num_batches, num_workers):\n",
    "        all_data = []\n",
    "        for worker_id in range(num_workers):\n",
    "            for batch_id in range(num_batches):\n",
    "                try:\n",
    "                    batch_data = Simulation.load_data_batch(batch_id, worker_id)\n",
    "                    all_data.extend(batch_data)\n",
    "                except FileNotFoundError:\n",
    "                    print(f\"Batch {batch_id} for worker {worker_id} not found.\")\n",
    "        return all_data\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_simulation(dataset, sim_number):\n",
    "        fig, axs = plt.subplots(4, 1, figsize=(12, 18))\n",
    "\n",
    "        wavelengths = dataset.keys()\n",
    "        for i, wavelength in enumerate(wavelengths):\n",
    "            xdata_unmasked = dataset[wavelength]['xdata_unmasked']\n",
    "            xdata_masked = dataset[wavelength]['xdata_masked']\n",
    "            ydata_unmasked = dataset[wavelength]['ydata_unmasked']\n",
    "            ydata_masked = dataset[wavelength]['ydata_masked']\n",
    "\n",
    "            axs[i].scatter(xdata_unmasked, ydata_unmasked, label='Unmasked', s=5)\n",
    "            axs[i].scatter(xdata_masked, ydata_masked, label='Masked', s=5)\n",
    "            \n",
    "            axs[i].set_title(f'{wavelength} Data')\n",
    "            axs[i].set_xlabel('Time')\n",
    "            axs[i].set_ylabel('Flux')\n",
    "            axs[i].legend()\n",
    "\n",
    "        fig.suptitle(f'Simulation {sim_number}', fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_all_simulations(all_data):\n",
    "        for i, dataset in enumerate(all_data):\n",
    "            Simulation.plot_simulation(dataset, i)\n",
    "\n",
    "def run_parallel_simulations(epoch, sampling_rate, VarDict, num_simulations, batch_size, num_workers):\n",
    "    # Divide the total number of simulations among workers\n",
    "    chunk_size = num_simulations // num_workers\n",
    "    ranges = [(i * chunk_size, (i + 1) * chunk_size) for i in range(num_workers)]\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = [\n",
    "            executor.submit(run_batch_for_worker, epoch, sampling_rate, VarDict, start, end, batch_size, worker_id)\n",
    "            for worker_id, (start, end) in enumerate(ranges)\n",
    "        ]\n",
    "        for future in futures:\n",
    "            future.result()  # Wait for all processes to complete\n",
    "\n",
    "def run_batch_for_worker(epoch, sampling_rate, VarDict, start, end, batch_size, worker_id):\n",
    "    # Create a separate Simulation instance for each worker\n",
    "    worker_simulation = Simulation(\n",
    "        epoch=epoch, \n",
    "        sampling_rate=sampling_rate, \n",
    "        VarDict=VarDict, \n",
    "        num_simulations=end - start, \n",
    "        batch_size=batch_size, \n",
    "        worker_id=worker_id\n",
    "    )\n",
    "    \n",
    "    worker_simulation.run_and_save_simulations()\n",
    "\n",
    "\n",
    "# Parameters for the simulation\n",
    "epoch = 960.0\n",
    "sampling_rate = 1\n",
    "num_simulations = 100000  # Adjust the number of simulations if needed\n",
    "batch_size = 50  # Batch size for saving\n",
    "VarDict = {\n",
    "    \"PSD_slope_fast\": \"fast_a1\", \"PSD_break_fast\": \"fast_b1\",\n",
    "    \"mu_fast\": \"fast_mu\", \"sig_fast\": \"fast_sig\", \"PSD_slope_slow\": \"a1\", \"PSD_break_slow\": \"b1\",\n",
    "    \"mu_slow\": \"mu\", \"sig_slow\": \"sig\", \"B_0\": \"B_0\", \"gamma\": \"gamma\", \"ampfac\": \"ampfac\",\n",
    "    \"size_0\": \"size_0\", \"noise_NIR\": \"vlt_noise\", \"X_offset\": \"I_offset\", \"rate_conv\": \"rate_conv\",\n",
    "    \"eff_area\": \"I_eff_area\", \"model_gain\": \"a\", \"f0_B\": \"f_0_B\", \"f0_theta\": \"f_0_size\",\n",
    "    \"noise_345GHz\": \"APEX_noise\", \"noise_230GHz\": \"SMA_noise\", \"noise_340GHz\": \"APEX_noise\"\n",
    "}\n",
    "\n",
    "# Run the parallel simulations with 4 workers\n",
    "num_workers = 4\n",
    "run_parallel_simulations(epoch, sampling_rate, VarDict, num_simulations, batch_size, num_workers)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
